{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Advanced Level","metadata":{}},{"cell_type":"markdown","source":"# Task 9 : Handwritten equation solver using CNN","metadata":{}},{"cell_type":"markdown","source":"<center><img src = \"https://images.unsplash.com/photo-1581089778245-3ce67677f718?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80\"></center>","metadata":{}},{"cell_type":"markdown","source":"<center><h1>➗ Handwritten Equation Solver ➗</h1></center>","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"print(\"Loading...\")\n\n# common libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\n\n# CV and Image\nimport cv2\nfrom PIL import Image\n\n# pickle\nimport pickle\n\n# keras\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import *\nfrom keras.layers import Input, Dense, Dropout, Flatten\nfrom keras.utils import np_utils\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nK.image_data_format()\n\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:14.116849Z","iopub.execute_input":"2023-09-30T05:49:14.117430Z","iopub.status.idle":"2023-09-30T05:49:15.705006Z","shell.execute_reply.started":"2023-09-30T05:49:14.117308Z","shell.execute_reply":"2023-09-30T05:49:15.704104Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Loading...\nDone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating Data","metadata":{}},{"cell_type":"code","source":"p = \"../input/handwritten-math-symbols/dataset/\"\n\nprint(\"These are the folders we'll be working with :\")\n\nfor f in os.listdir(p):\n    print(f)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:15.706785Z","iopub.execute_input":"2023-09-30T05:49:15.707416Z","iopub.status.idle":"2023-09-30T05:49:15.714494Z","shell.execute_reply.started":"2023-09-30T05:49:15.707380Z","shell.execute_reply":"2023-09-30T05:49:15.713632Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"These are the folders we'll be working with :\n7\n2\n5\ndiv\n8\nx\n0\ny\nz\nadd\n3\neq\ndec\nsub\n1\n4\n9\nmul\n6\n.directory\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_images(folder):\n    \n    train_data=[]\n    \n    for filename in os.listdir(folder):\n        \n        if filename != \".directory\" :\n\n            image = cv2.imread(os.path.join(folder,filename),cv2.IMREAD_GRAYSCALE)\n            image = ~image\n\n            if image is not None:\n\n                ret, thresh = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n                contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n                contour = sorted(contours, key = lambda ctr: cv2.boundingRect(ctr)[0])\n\n                a = int(28)\n                b = int(28)\n                maxi = 0\n\n                for c in contour:\n\n                    x,y,a,b=cv2.boundingRect(c)\n                    maxi=max(a*b,maxi)\n\n                    if maxi==a*b:\n\n                        x_max=x\n                        y_max=y\n                        w_max=a\n                        h_max=b\n\n                im_crop = thresh[y_max:y_max+h_max+10, x_max:x_max+w_max+10]\n                im_resize = cv2.resize(im_crop,(28,28))\n                im_resize = np.reshape(im_resize,(784,1))\n                train_data.append(im_resize)\n            \n    return train_data","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:15.715947Z","iopub.execute_input":"2023-09-30T05:49:15.716274Z","iopub.status.idle":"2023-09-30T05:49:15.727351Z","shell.execute_reply.started":"2023-09-30T05:49:15.716242Z","shell.execute_reply":"2023-09-30T05:49:15.726507Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Concatenating Data of All Characters","metadata":{}},{"cell_type":"code","source":"data = []\n\ndata = load_images(p+\"0\")\nfor i in range(0, len(data)):\n    data[i] = np.append(data[i], ['0'])\nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:15.729251Z","iopub.execute_input":"2023-09-30T05:49:15.729836Z","iopub.status.idle":"2023-09-30T05:49:16.805113Z","shell.execute_reply.started":"2023-09-30T05:49:15.729801Z","shell.execute_reply":"2023-09-30T05:49:16.804189Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"595\n","output_type":"stream"}]},{"cell_type":"code","source":"iter = 0\n\nfor i in list(os.listdir(p)) :\n    \n    if i not in [\"0\", \".directory\"] :\n    \n        print(\"Iter :\", iter)\n        print(\"Working with the\", i, \"folder\")\n\n        data_i = load_images(p+i)\n\n        if i in [str(k) for k in range(1, 10)] :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [i])\n\n        if i == \"add\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"10\"])\n\n        if i == \"sub\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"11\"])\n\n\n        if i == \"mul\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"12\"])\n\n        if i == \"div\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"13\"])\n\n        if i == \"eq\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"14\"])                \n\n        if i == \"dec\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"15\"])\n\n        if i == \"x\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"16\"])\n\n        if i == \"y\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"17\"]) \n\n\n        if i == \"z\" :\n\n            for j in range(0, len(data_i)):\n                data_i[j] = np.append(data_i[j], [\"18\"]) \n\n\n        data = np.concatenate((data, data_i))\n        print(len(data))\n\n        print()\n\n        iter += 1","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:16.806530Z","iopub.execute_input":"2023-09-30T05:49:16.807160Z","iopub.status.idle":"2023-09-30T05:49:35.220907Z","shell.execute_reply.started":"2023-09-30T05:49:16.807122Z","shell.execute_reply":"2023-09-30T05:49:35.220002Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Iter : 0\nWorking with the 7 folder\n1128\n\nIter : 1\nWorking with the 2 folder\n1561\n\nIter : 2\nWorking with the 5 folder\n1994\n\nIter : 3\nWorking with the div folder\n2612\n\nIter : 4\nWorking with the 8 folder\n3166\n\nIter : 5\nWorking with the x folder\n3618\n\nIter : 6\nWorking with the y folder\n4017\n\nIter : 7\nWorking with the z folder\n4229\n\nIter : 8\nWorking with the add folder\n4825\n\nIter : 9\nWorking with the 3 folder\n5366\n\nIter : 10\nWorking with the eq folder\n6000\n\nIter : 11\nWorking with the dec folder\n6624\n\nIter : 12\nWorking with the sub folder\n7279\n\nIter : 13\nWorking with the 1 folder\n7841\n\nIter : 14\nWorking with the 4 folder\n8367\n\nIter : 15\nWorking with the 9 folder\n8913\n\nIter : 16\nWorking with the mul folder\n9490\n\nIter : 17\nWorking with the 6 folder\n10071\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating Training set","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame(data,index=None)\ndf.to_csv('train_handwritten.csv',index=False)\n\ndata = pd.read_csv('train_handwritten.csv',index_col=False)\nlabels = data[['784']]\n\ndata.drop(data.columns[[784]],axis=1,inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:35.222374Z","iopub.execute_input":"2023-09-30T05:49:35.222771Z","iopub.status.idle":"2023-09-30T05:49:37.276277Z","shell.execute_reply.started":"2023-09-30T05:49:35.222733Z","shell.execute_reply":"2023-09-30T05:49:37.275375Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   0  1    2    3    4    5    6    7    8    9  ...  774  775  776  777  778  \\\n0  0  0    0    0   18  255  255  255  255  255  ...    0    0    0    0    0   \n1  0  0    0    0    1  255  255  255  255  255  ...    0    0    0    0    0   \n2  0  0    0    0    0    0  113  132  185  255  ...    0    0    0    0    0   \n3  0  0    0    0    0    0    0  255  255  255  ...    0    0    0    0    0   \n4  0  0  198  255  255  255  255  111    0    0  ...    0    0    0    0    0   \n\n   779  780  781  782  783  \n0    0    0    0    0    0  \n1    0    0    0    0    0  \n2    0    0    0    0    0  \n3    0    0    0    0    0  \n4    0    0    0    0    0  \n\n[5 rows x 784 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>774</th>\n      <th>775</th>\n      <th>776</th>\n      <th>777</th>\n      <th>778</th>\n      <th>779</th>\n      <th>780</th>\n      <th>781</th>\n      <th>782</th>\n      <th>783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113</td>\n      <td>132</td>\n      <td>185</td>\n      <td>255</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>198</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>255</td>\n      <td>111</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 784 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"np.random.seed(1212)\nlabels=np.array(labels)\ncat=to_categorical(labels,num_classes=19)\ncat[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:37.277544Z","iopub.execute_input":"2023-09-30T05:49:37.278532Z","iopub.status.idle":"2023-09-30T05:49:37.287710Z","shell.execute_reply.started":"2023-09-30T05:49:37.278487Z","shell.execute_reply":"2023-09-30T05:49:37.286633Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"temp=data.to_numpy()\nX_train = temp.reshape(temp.shape[0], 28, 28, 1)\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:37.288985Z","iopub.execute_input":"2023-09-30T05:49:37.290189Z","iopub.status.idle":"2023-09-30T05:49:37.300656Z","shell.execute_reply.started":"2023-09-30T05:49:37.290088Z","shell.execute_reply":"2023-09-30T05:49:37.299514Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(10071, 28, 28, 1)"},"metadata":{}}]},{"cell_type":"code","source":"l=[]\nfor i in range(X_train.shape[0]):\n    l.append(np.array(data[i:i+1]).reshape(1,28,28))\n\nnp.random.seed(7)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:37.301887Z","iopub.execute_input":"2023-09-30T05:49:37.302897Z","iopub.status.idle":"2023-09-30T05:49:37.737379Z","shell.execute_reply.started":"2023-09-30T05:49:37.302852Z","shell.execute_reply":"2023-09-30T05:49:37.736565Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3,3), input_shape=(28, 28, 1), activation='relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(15, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(19, activation='softmax'))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-30T05:49:37.739937Z","iopub.execute_input":"2023-09-30T05:49:37.740484Z","iopub.status.idle":"2023-09-30T05:49:39.102187Z","shell.execute_reply.started":"2023-09-30T05:49:37.740445Z","shell.execute_reply":"2023-09-30T05:49:39.101365Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:39.103457Z","iopub.execute_input":"2023-09-30T05:49:39.103964Z","iopub.status.idle":"2023-09-30T05:49:39.110768Z","shell.execute_reply.started":"2023-09-30T05:49:39.103930Z","shell.execute_reply":"2023-09-30T05:49:39.109812Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 28, 28, 32)        320       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 12, 12, 15)        4335      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 15)          0         \n_________________________________________________________________\ndropout (Dropout)            (None, 6, 6, 15)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 540)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               69248     \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                6450      \n_________________________________________________________________\ndense_2 (Dense)              (None, 19)                969       \n=================================================================\nTotal params: 81,322\nTrainable params: 81,322\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"plot_model(model, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:49:39.112329Z","iopub.execute_input":"2023-09-30T05:49:39.112880Z","iopub.status.idle":"2023-09-30T05:49:39.277803Z","shell.execute_reply.started":"2023-09-30T05:49:39.112845Z","shell.execute_reply":"2023-09-30T05:49:39.276749Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"image/png":"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\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"eq_solver.h5\", monitor='accuracy', verbose=1, save_best_only=True)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\nnn = model.fit(X_train, cat, epochs=1000, batch_size=256, shuffle=True, verbose=1, callbacks=[checkpoint]).history","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2023-09-30T05:49:39.279374Z","iopub.execute_input":"2023-09-30T05:49:39.280478Z","iopub.status.idle":"2023-09-30T05:53:01.851865Z","shell.execute_reply.started":"2023-09-30T05:49:39.280437Z","shell.execute_reply":"2023-09-30T05:53:01.850910Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n40/40 [==============================] - 2s 5ms/step - loss: 6.9297 - accuracy: 0.1661\n\nEpoch 00001: accuracy improved from -inf to 0.16612, saving model to eq_solver.h5\nEpoch 2/1000\n40/40 [==============================] - 0s 4ms/step - loss: 1.8256 - accuracy: 0.4416\n\nEpoch 00002: accuracy improved from 0.16612 to 0.44156, saving model to eq_solver.h5\nEpoch 3/1000\n40/40 [==============================] - 0s 4ms/step - loss: 1.0180 - accuracy: 0.6770\n\nEpoch 00003: accuracy improved from 0.44156 to 0.67699, saving model to eq_solver.h5\nEpoch 4/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.7792\n\nEpoch 00004: accuracy improved from 0.67699 to 0.77917, saving model to eq_solver.h5\nEpoch 5/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.5201 - accuracy: 0.8206\n\nEpoch 00005: accuracy improved from 0.77917 to 0.82057, saving model to eq_solver.h5\nEpoch 6/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.4382 - accuracy: 0.8438\n\nEpoch 00006: accuracy improved from 0.82057 to 0.84381, saving model to eq_solver.h5\nEpoch 7/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.3651 - accuracy: 0.8689\n\nEpoch 00007: accuracy improved from 0.84381 to 0.86893, saving model to eq_solver.h5\nEpoch 8/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.8780\n\nEpoch 00008: accuracy improved from 0.86893 to 0.87797, saving model to eq_solver.h5\nEpoch 9/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.2876 - accuracy: 0.8955\n\nEpoch 00009: accuracy improved from 0.87797 to 0.89554, saving model to eq_solver.h5\nEpoch 10/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.2691 - accuracy: 0.8986\n\nEpoch 00010: accuracy improved from 0.89554 to 0.89862, saving model to eq_solver.h5\nEpoch 11/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.2550 - accuracy: 0.9077\n\nEpoch 00011: accuracy improved from 0.89862 to 0.90766, saving model to eq_solver.h5\nEpoch 12/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.2325 - accuracy: 0.9130\n\nEpoch 00012: accuracy improved from 0.90766 to 0.91302, saving model to eq_solver.h5\nEpoch 13/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.2324 - accuracy: 0.9119\n\nEpoch 00013: accuracy did not improve from 0.91302\nEpoch 14/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.2095 - accuracy: 0.9197\n\nEpoch 00014: accuracy improved from 0.91302 to 0.91967, saving model to eq_solver.h5\nEpoch 15/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1985 - accuracy: 0.9225\n\nEpoch 00015: accuracy improved from 0.91967 to 0.92255, saving model to eq_solver.h5\nEpoch 16/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9264\n\nEpoch 00016: accuracy improved from 0.92255 to 0.92642, saving model to eq_solver.h5\nEpoch 17/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.9262\n\nEpoch 00017: accuracy did not improve from 0.92642\nEpoch 18/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1775 - accuracy: 0.9294\n\nEpoch 00018: accuracy improved from 0.92642 to 0.92940, saving model to eq_solver.h5\nEpoch 19/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9308\n\nEpoch 00019: accuracy improved from 0.92940 to 0.93079, saving model to eq_solver.h5\nEpoch 20/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9372\n\nEpoch 00020: accuracy improved from 0.93079 to 0.93725, saving model to eq_solver.h5\nEpoch 21/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9367\n\nEpoch 00021: accuracy did not improve from 0.93725\nEpoch 22/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9382\n\nEpoch 00022: accuracy improved from 0.93725 to 0.93824, saving model to eq_solver.h5\nEpoch 23/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9382\n\nEpoch 00023: accuracy did not improve from 0.93824\nEpoch 24/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9436\n\nEpoch 00024: accuracy improved from 0.93824 to 0.94360, saving model to eq_solver.h5\nEpoch 25/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9445\n\nEpoch 00025: accuracy improved from 0.94360 to 0.94449, saving model to eq_solver.h5\nEpoch 26/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9437\n\nEpoch 00026: accuracy did not improve from 0.94449\nEpoch 27/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1368 - accuracy: 0.9467\n\nEpoch 00027: accuracy improved from 0.94449 to 0.94668, saving model to eq_solver.h5\nEpoch 28/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9487\n\nEpoch 00028: accuracy improved from 0.94668 to 0.94866, saving model to eq_solver.h5\nEpoch 29/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1289 - accuracy: 0.9506\n\nEpoch 00029: accuracy improved from 0.94866 to 0.95055, saving model to eq_solver.h5\nEpoch 30/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.9497\n\nEpoch 00030: accuracy did not improve from 0.95055\nEpoch 31/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1224 - accuracy: 0.9509\n\nEpoch 00031: accuracy improved from 0.95055 to 0.95095, saving model to eq_solver.h5\nEpoch 32/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9520\n\nEpoch 00032: accuracy improved from 0.95095 to 0.95204, saving model to eq_solver.h5\nEpoch 33/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9537\n\nEpoch 00033: accuracy improved from 0.95204 to 0.95373, saving model to eq_solver.h5\nEpoch 34/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9517\n\nEpoch 00034: accuracy did not improve from 0.95373\nEpoch 35/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9517\n\nEpoch 00035: accuracy did not improve from 0.95373\nEpoch 36/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9564\n\nEpoch 00036: accuracy improved from 0.95373 to 0.95641, saving model to eq_solver.h5\nEpoch 37/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9570\n\nEpoch 00037: accuracy improved from 0.95641 to 0.95701, saving model to eq_solver.h5\nEpoch 38/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9556\n\nEpoch 00038: accuracy did not improve from 0.95701\nEpoch 39/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9573\n\nEpoch 00039: accuracy improved from 0.95701 to 0.95730, saving model to eq_solver.h5\nEpoch 40/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9567\n\nEpoch 00040: accuracy did not improve from 0.95730\nEpoch 41/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1077 - accuracy: 0.9577\n\nEpoch 00041: accuracy improved from 0.95730 to 0.95770, saving model to eq_solver.h5\nEpoch 42/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9608\n\nEpoch 00042: accuracy improved from 0.95770 to 0.96078, saving model to eq_solver.h5\nEpoch 43/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.9586\n\nEpoch 00043: accuracy did not improve from 0.96078\nEpoch 44/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1023 - accuracy: 0.9602\n\nEpoch 00044: accuracy did not improve from 0.96078\nEpoch 45/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.1023 - accuracy: 0.9599\n\nEpoch 00045: accuracy did not improve from 0.96078\nEpoch 46/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9625\n\nEpoch 00046: accuracy improved from 0.96078 to 0.96247, saving model to eq_solver.h5\nEpoch 47/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9609\n\nEpoch 00047: accuracy did not improve from 0.96247\nEpoch 48/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9628\n\nEpoch 00048: accuracy improved from 0.96247 to 0.96276, saving model to eq_solver.h5\nEpoch 49/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0928 - accuracy: 0.9636\n\nEpoch 00049: accuracy improved from 0.96276 to 0.96356, saving model to eq_solver.h5\nEpoch 50/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.9637\n\nEpoch 00050: accuracy improved from 0.96356 to 0.96366, saving model to eq_solver.h5\nEpoch 51/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9656\n\nEpoch 00051: accuracy improved from 0.96366 to 0.96564, saving model to eq_solver.h5\nEpoch 52/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9643\n\nEpoch 00052: accuracy did not improve from 0.96564\nEpoch 53/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0907 - accuracy: 0.9628\n\nEpoch 00053: accuracy did not improve from 0.96564\nEpoch 54/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9673\n\nEpoch 00054: accuracy improved from 0.96564 to 0.96733, saving model to eq_solver.h5\nEpoch 55/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9690\n\nEpoch 00055: accuracy improved from 0.96733 to 0.96902, saving model to eq_solver.h5\nEpoch 56/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9691\n\nEpoch 00056: accuracy improved from 0.96902 to 0.96912, saving model to eq_solver.h5\nEpoch 57/1000\n40/40 [==============================] - 0s 8ms/step - loss: 0.0800 - accuracy: 0.9693\n\nEpoch 00057: accuracy improved from 0.96912 to 0.96932, saving model to eq_solver.h5\nEpoch 58/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9696\n\nEpoch 00058: accuracy improved from 0.96932 to 0.96962, saving model to eq_solver.h5\nEpoch 59/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0754 - accuracy: 0.9713\n\nEpoch 00059: accuracy improved from 0.96962 to 0.97130, saving model to eq_solver.h5\nEpoch 60/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9704\n\nEpoch 00060: accuracy did not improve from 0.97130\nEpoch 61/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.9712\n\nEpoch 00061: accuracy did not improve from 0.97130\nEpoch 62/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9710\n\nEpoch 00062: accuracy did not improve from 0.97130\nEpoch 63/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0712 - accuracy: 0.9709\n\nEpoch 00063: accuracy did not improve from 0.97130\nEpoch 64/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9708\n\nEpoch 00064: accuracy did not improve from 0.97130\nEpoch 65/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9698\n\nEpoch 00065: accuracy did not improve from 0.97130\nEpoch 66/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9728\n\nEpoch 00066: accuracy improved from 0.97130 to 0.97279, saving model to eq_solver.h5\nEpoch 67/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9727\n\nEpoch 00067: accuracy did not improve from 0.97279\nEpoch 68/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9751\n\nEpoch 00068: accuracy improved from 0.97279 to 0.97508, saving model to eq_solver.h5\nEpoch 69/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9744\n\nEpoch 00069: accuracy did not improve from 0.97508\nEpoch 70/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9750\n\nEpoch 00070: accuracy did not improve from 0.97508\nEpoch 71/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.9761\n\nEpoch 00071: accuracy improved from 0.97508 to 0.97607, saving model to eq_solver.h5\nEpoch 72/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9770\n\nEpoch 00072: accuracy improved from 0.97607 to 0.97696, saving model to eq_solver.h5\nEpoch 73/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.9762\n\nEpoch 00073: accuracy did not improve from 0.97696\nEpoch 74/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9788\n\nEpoch 00074: accuracy improved from 0.97696 to 0.97875, saving model to eq_solver.h5\nEpoch 75/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.9748\n\nEpoch 00075: accuracy did not improve from 0.97875\nEpoch 76/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9762\n\nEpoch 00076: accuracy did not improve from 0.97875\nEpoch 77/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9751\n\nEpoch 00077: accuracy did not improve from 0.97875\nEpoch 78/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9749\n\nEpoch 00078: accuracy did not improve from 0.97875\nEpoch 79/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9742\n\nEpoch 00079: accuracy did not improve from 0.97875\nEpoch 80/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0712 - accuracy: 0.9748\n\nEpoch 00080: accuracy did not improve from 0.97875\nEpoch 81/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0692 - accuracy: 0.9745\n\nEpoch 00081: accuracy did not improve from 0.97875\nEpoch 82/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.9782\n\nEpoch 00082: accuracy did not improve from 0.97875\nEpoch 83/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.9771\n\nEpoch 00083: accuracy did not improve from 0.97875\nEpoch 84/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9732\n\nEpoch 00084: accuracy did not improve from 0.97875\nEpoch 85/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.9742\n\nEpoch 00085: accuracy did not improve from 0.97875\nEpoch 86/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.9781\n\nEpoch 00086: accuracy did not improve from 0.97875\nEpoch 87/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9782\n\nEpoch 00087: accuracy did not improve from 0.97875\nEpoch 88/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9790\n\nEpoch 00088: accuracy improved from 0.97875 to 0.97905, saving model to eq_solver.h5\nEpoch 89/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9808\n\nEpoch 00089: accuracy improved from 0.97905 to 0.98084, saving model to eq_solver.h5\nEpoch 90/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9800\n\nEpoch 00090: accuracy did not improve from 0.98084\nEpoch 91/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0513 - accuracy: 0.9816\n\nEpoch 00091: accuracy improved from 0.98084 to 0.98163, saving model to eq_solver.h5\nEpoch 92/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9744\n\nEpoch 00092: accuracy did not improve from 0.98163\nEpoch 93/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9809\n\nEpoch 00093: accuracy did not improve from 0.98163\nEpoch 94/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9823\n\nEpoch 00094: accuracy improved from 0.98163 to 0.98233, saving model to eq_solver.h5\nEpoch 95/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9808\n\nEpoch 00095: accuracy did not improve from 0.98233\nEpoch 96/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9799\n\nEpoch 00096: accuracy did not improve from 0.98233\nEpoch 97/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9821\n\nEpoch 00097: accuracy did not improve from 0.98233\nEpoch 98/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0474 - accuracy: 0.9830\n\nEpoch 00098: accuracy improved from 0.98233 to 0.98302, saving model to eq_solver.h5\nEpoch 99/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9807\n\nEpoch 00099: accuracy did not improve from 0.98302\nEpoch 100/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9845\n\nEpoch 00100: accuracy improved from 0.98302 to 0.98451, saving model to eq_solver.h5\nEpoch 101/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9836\n\nEpoch 00101: accuracy did not improve from 0.98451\nEpoch 102/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9840\n\nEpoch 00102: accuracy did not improve from 0.98451\nEpoch 103/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9831\n\nEpoch 00103: accuracy did not improve from 0.98451\nEpoch 104/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9825\n\nEpoch 00104: accuracy did not improve from 0.98451\nEpoch 105/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9825\n\nEpoch 00105: accuracy did not improve from 0.98451\nEpoch 106/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0461 - accuracy: 0.9819\n\nEpoch 00106: accuracy did not improve from 0.98451\nEpoch 107/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9815\n\nEpoch 00107: accuracy did not improve from 0.98451\nEpoch 108/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9839\n\nEpoch 00108: accuracy did not improve from 0.98451\nEpoch 109/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9851\n\nEpoch 00109: accuracy improved from 0.98451 to 0.98511, saving model to eq_solver.h5\nEpoch 110/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9875\n\nEpoch 00110: accuracy improved from 0.98511 to 0.98749, saving model to eq_solver.h5\nEpoch 111/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.9839\n\nEpoch 00111: accuracy did not improve from 0.98749\nEpoch 112/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9812\n\nEpoch 00112: accuracy did not improve from 0.98749\nEpoch 113/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 0.9827\n\nEpoch 00113: accuracy did not improve from 0.98749\nEpoch 114/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9838\n\nEpoch 00114: accuracy did not improve from 0.98749\nEpoch 115/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9867\n\nEpoch 00115: accuracy did not improve from 0.98749\nEpoch 116/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9842\n\nEpoch 00116: accuracy did not improve from 0.98749\nEpoch 117/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0380 - accuracy: 0.9866\n\nEpoch 00117: accuracy did not improve from 0.98749\nEpoch 118/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9841\n\nEpoch 00118: accuracy did not improve from 0.98749\nEpoch 119/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9843\n\nEpoch 00119: accuracy did not improve from 0.98749\nEpoch 120/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9846\n\nEpoch 00120: accuracy did not improve from 0.98749\nEpoch 121/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9839\n\nEpoch 00121: accuracy did not improve from 0.98749\nEpoch 122/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9865\n\nEpoch 00122: accuracy did not improve from 0.98749\nEpoch 123/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.9857\n\nEpoch 00123: accuracy did not improve from 0.98749\nEpoch 124/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9867\n\nEpoch 00124: accuracy did not improve from 0.98749\nEpoch 125/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9889\n\nEpoch 00125: accuracy improved from 0.98749 to 0.98888, saving model to eq_solver.h5\nEpoch 126/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9883\n\nEpoch 00126: accuracy did not improve from 0.98888\nEpoch 127/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.9868\n\nEpoch 00127: accuracy did not improve from 0.98888\nEpoch 128/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.9864\n\nEpoch 00128: accuracy did not improve from 0.98888\nEpoch 129/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.9861\n\nEpoch 00129: accuracy did not improve from 0.98888\nEpoch 130/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0407 - accuracy: 0.9856\n\nEpoch 00130: accuracy did not improve from 0.98888\nEpoch 131/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.9854\n\nEpoch 00131: accuracy did not improve from 0.98888\nEpoch 132/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0402 - accuracy: 0.9852\n\nEpoch 00132: accuracy did not improve from 0.98888\nEpoch 133/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9862\n\nEpoch 00133: accuracy did not improve from 0.98888\nEpoch 134/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9884\n\nEpoch 00134: accuracy did not improve from 0.98888\nEpoch 135/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9887\n\nEpoch 00135: accuracy did not improve from 0.98888\nEpoch 136/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0327 - accuracy: 0.9876\n\nEpoch 00136: accuracy did not improve from 0.98888\nEpoch 137/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.9909\n\nEpoch 00137: accuracy improved from 0.98888 to 0.99086, saving model to eq_solver.h5\nEpoch 138/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9892\n\nEpoch 00138: accuracy did not improve from 0.99086\nEpoch 139/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.9883\n\nEpoch 00139: accuracy did not improve from 0.99086\nEpoch 140/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9871\n\nEpoch 00140: accuracy did not improve from 0.99086\nEpoch 141/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9874\n\nEpoch 00141: accuracy did not improve from 0.99086\nEpoch 142/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9870\n\nEpoch 00142: accuracy did not improve from 0.99086\nEpoch 143/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9875\n\nEpoch 00143: accuracy did not improve from 0.99086\nEpoch 144/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0447 - accuracy: 0.9843\n\nEpoch 00144: accuracy did not improve from 0.99086\nEpoch 145/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9869\n\nEpoch 00145: accuracy did not improve from 0.99086\nEpoch 146/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9880\n\nEpoch 00146: accuracy did not improve from 0.99086\nEpoch 147/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9893\n\nEpoch 00147: accuracy did not improve from 0.99086\nEpoch 148/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9896\n\nEpoch 00148: accuracy did not improve from 0.99086\nEpoch 149/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9860\n\nEpoch 00149: accuracy did not improve from 0.99086\nEpoch 150/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9896\n\nEpoch 00150: accuracy did not improve from 0.99086\nEpoch 151/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9897\n\nEpoch 00151: accuracy did not improve from 0.99086\nEpoch 152/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.9902\n\nEpoch 00152: accuracy did not improve from 0.99086\nEpoch 153/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9896\n\nEpoch 00153: accuracy did not improve from 0.99086\nEpoch 154/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9878\n\nEpoch 00154: accuracy did not improve from 0.99086\nEpoch 155/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 0.9890\n\nEpoch 00155: accuracy did not improve from 0.99086\nEpoch 156/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9882\n\nEpoch 00156: accuracy did not improve from 0.99086\nEpoch 157/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9884\n\nEpoch 00157: accuracy did not improve from 0.99086\nEpoch 158/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.9902\n\nEpoch 00158: accuracy did not improve from 0.99086\nEpoch 159/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9902\n\nEpoch 00159: accuracy did not improve from 0.99086\nEpoch 160/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9897\n\nEpoch 00160: accuracy did not improve from 0.99086\nEpoch 161/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.9895\n\nEpoch 00161: accuracy did not improve from 0.99086\nEpoch 162/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9893\n\nEpoch 00162: accuracy did not improve from 0.99086\nEpoch 163/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9890\n\nEpoch 00163: accuracy did not improve from 0.99086\nEpoch 164/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 0.9893\n\nEpoch 00164: accuracy did not improve from 0.99086\nEpoch 165/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9882\n\nEpoch 00165: accuracy did not improve from 0.99086\nEpoch 166/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9884\n\nEpoch 00166: accuracy did not improve from 0.99086\nEpoch 167/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9897\n\nEpoch 00167: accuracy did not improve from 0.99086\nEpoch 168/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9906\n\nEpoch 00168: accuracy did not improve from 0.99086\nEpoch 169/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 0.9903\n\nEpoch 00169: accuracy did not improve from 0.99086\nEpoch 170/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.9899\n\nEpoch 00170: accuracy did not improve from 0.99086\nEpoch 171/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9911\n\nEpoch 00171: accuracy improved from 0.99086 to 0.99106, saving model to eq_solver.h5\nEpoch 172/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9917\n\nEpoch 00172: accuracy improved from 0.99106 to 0.99166, saving model to eq_solver.h5\nEpoch 173/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.9916\n\nEpoch 00173: accuracy did not improve from 0.99166\nEpoch 174/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9924\n\nEpoch 00174: accuracy improved from 0.99166 to 0.99235, saving model to eq_solver.h5\nEpoch 175/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.9916\n\nEpoch 00175: accuracy did not improve from 0.99235\nEpoch 176/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9916\n\nEpoch 00176: accuracy did not improve from 0.99235\nEpoch 177/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9927\n\nEpoch 00177: accuracy improved from 0.99235 to 0.99265, saving model to eq_solver.h5\nEpoch 178/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.9924\n\nEpoch 00178: accuracy did not improve from 0.99265\nEpoch 179/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9942\n\nEpoch 00179: accuracy improved from 0.99265 to 0.99424, saving model to eq_solver.h5\nEpoch 180/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0350 - accuracy: 0.9895\n\nEpoch 00180: accuracy did not improve from 0.99424\nEpoch 181/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9908\n\nEpoch 00181: accuracy did not improve from 0.99424\nEpoch 182/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.9919\n\nEpoch 00182: accuracy did not improve from 0.99424\nEpoch 183/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.9891\n\nEpoch 00183: accuracy did not improve from 0.99424\nEpoch 184/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9901\n\nEpoch 00184: accuracy did not improve from 0.99424\nEpoch 185/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9907\n\nEpoch 00185: accuracy did not improve from 0.99424\nEpoch 186/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.9911\n\nEpoch 00186: accuracy did not improve from 0.99424\nEpoch 187/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9913\n\nEpoch 00187: accuracy did not improve from 0.99424\nEpoch 188/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9905\n\nEpoch 00188: accuracy did not improve from 0.99424\nEpoch 189/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9928\n\nEpoch 00189: accuracy did not improve from 0.99424\nEpoch 190/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.9904\n\nEpoch 00190: accuracy did not improve from 0.99424\nEpoch 191/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9924\n\nEpoch 00191: accuracy did not improve from 0.99424\nEpoch 192/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9905\n\nEpoch 00192: accuracy did not improve from 0.99424\nEpoch 193/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9900\n\nEpoch 00193: accuracy did not improve from 0.99424\nEpoch 194/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9930\n\nEpoch 00194: accuracy did not improve from 0.99424\nEpoch 195/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9894\n\nEpoch 00195: accuracy did not improve from 0.99424\nEpoch 196/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9928\n\nEpoch 00196: accuracy did not improve from 0.99424\nEpoch 197/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9937\n\nEpoch 00197: accuracy did not improve from 0.99424\nEpoch 198/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9942\n\nEpoch 00198: accuracy did not improve from 0.99424\nEpoch 199/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.9923\n\nEpoch 00199: accuracy did not improve from 0.99424\nEpoch 200/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9910\n\nEpoch 00200: accuracy did not improve from 0.99424\nEpoch 201/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9923\n\nEpoch 00201: accuracy did not improve from 0.99424\nEpoch 202/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9933\n\nEpoch 00202: accuracy did not improve from 0.99424\nEpoch 203/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9905\n\nEpoch 00203: accuracy did not improve from 0.99424\nEpoch 204/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.9917\n\nEpoch 00204: accuracy did not improve from 0.99424\nEpoch 205/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9926\n\nEpoch 00205: accuracy did not improve from 0.99424\nEpoch 206/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9889\n\nEpoch 00206: accuracy did not improve from 0.99424\nEpoch 207/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9890\n\nEpoch 00207: accuracy did not improve from 0.99424\nEpoch 208/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9910\n\nEpoch 00208: accuracy did not improve from 0.99424\nEpoch 209/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9929\n\nEpoch 00209: accuracy did not improve from 0.99424\nEpoch 210/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.9917\n\nEpoch 00210: accuracy did not improve from 0.99424\nEpoch 211/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9932\n\nEpoch 00211: accuracy did not improve from 0.99424\nEpoch 212/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9938\n\nEpoch 00212: accuracy did not improve from 0.99424\nEpoch 213/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.9917\n\nEpoch 00213: accuracy did not improve from 0.99424\nEpoch 214/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9939\n\nEpoch 00214: accuracy did not improve from 0.99424\nEpoch 215/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9940\n\nEpoch 00215: accuracy did not improve from 0.99424\nEpoch 216/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9924\n\nEpoch 00216: accuracy did not improve from 0.99424\nEpoch 217/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 0.9936\n\nEpoch 00217: accuracy did not improve from 0.99424\nEpoch 218/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9918\n\nEpoch 00218: accuracy did not improve from 0.99424\nEpoch 219/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.9931\n\nEpoch 00219: accuracy did not improve from 0.99424\nEpoch 220/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9920\n\nEpoch 00220: accuracy did not improve from 0.99424\nEpoch 221/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9944\n\nEpoch 00221: accuracy improved from 0.99424 to 0.99444, saving model to eq_solver.h5\nEpoch 222/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.9922\n\nEpoch 00222: accuracy did not improve from 0.99444\nEpoch 223/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.9940\n\nEpoch 00223: accuracy did not improve from 0.99444\nEpoch 224/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.9922\n\nEpoch 00224: accuracy did not improve from 0.99444\nEpoch 225/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9913\n\nEpoch 00225: accuracy did not improve from 0.99444\nEpoch 226/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 0.9924\n\nEpoch 00226: accuracy did not improve from 0.99444\nEpoch 227/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.9952\n\nEpoch 00227: accuracy improved from 0.99444 to 0.99523, saving model to eq_solver.h5\nEpoch 228/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 0.9941\n\nEpoch 00228: accuracy did not improve from 0.99523\nEpoch 229/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 0.9939\n\nEpoch 00229: accuracy did not improve from 0.99523\nEpoch 230/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0377 - accuracy: 0.9906\n\nEpoch 00230: accuracy did not improve from 0.99523\nEpoch 231/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 0.9911\n\nEpoch 00231: accuracy did not improve from 0.99523\nEpoch 232/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 0.9933\n\nEpoch 00232: accuracy did not improve from 0.99523\nEpoch 233/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9940\n\nEpoch 00233: accuracy did not improve from 0.99523\nEpoch 234/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9935\n\nEpoch 00234: accuracy did not improve from 0.99523\nEpoch 235/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9915\n\nEpoch 00235: accuracy did not improve from 0.99523\nEpoch 236/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 0.9929\n\nEpoch 00236: accuracy did not improve from 0.99523\nEpoch 237/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9938\n\nEpoch 00237: accuracy did not improve from 0.99523\nEpoch 238/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.9938\n\nEpoch 00238: accuracy did not improve from 0.99523\nEpoch 239/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9928\n\nEpoch 00239: accuracy did not improve from 0.99523\nEpoch 240/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9924\n\nEpoch 00240: accuracy did not improve from 0.99523\nEpoch 241/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9939\n\nEpoch 00241: accuracy did not improve from 0.99523\nEpoch 242/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9944\n\nEpoch 00242: accuracy did not improve from 0.99523\nEpoch 243/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9945\n\nEpoch 00243: accuracy did not improve from 0.99523\nEpoch 244/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.9930\n\nEpoch 00244: accuracy did not improve from 0.99523\nEpoch 245/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9938\n\nEpoch 00245: accuracy did not improve from 0.99523\nEpoch 246/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9940\n\nEpoch 00246: accuracy did not improve from 0.99523\nEpoch 247/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.9941\n\nEpoch 00247: accuracy did not improve from 0.99523\nEpoch 248/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9935\n\nEpoch 00248: accuracy did not improve from 0.99523\nEpoch 249/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9936\n\nEpoch 00249: accuracy did not improve from 0.99523\nEpoch 250/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9946\n\nEpoch 00250: accuracy did not improve from 0.99523\nEpoch 251/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9948\n\nEpoch 00251: accuracy did not improve from 0.99523\nEpoch 252/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9932\n\nEpoch 00252: accuracy did not improve from 0.99523\nEpoch 253/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9944\n\nEpoch 00253: accuracy did not improve from 0.99523\nEpoch 254/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9956\n\nEpoch 00254: accuracy improved from 0.99523 to 0.99563, saving model to eq_solver.h5\nEpoch 255/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9948\n\nEpoch 00255: accuracy did not improve from 0.99563\nEpoch 256/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.9930\n\nEpoch 00256: accuracy did not improve from 0.99563\nEpoch 257/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9942\n\nEpoch 00257: accuracy did not improve from 0.99563\nEpoch 258/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9938\n\nEpoch 00258: accuracy did not improve from 0.99563\nEpoch 259/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9967\n\nEpoch 00259: accuracy improved from 0.99563 to 0.99672, saving model to eq_solver.h5\nEpoch 260/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9941\n\nEpoch 00260: accuracy did not improve from 0.99672\nEpoch 261/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9943\n\nEpoch 00261: accuracy did not improve from 0.99672\nEpoch 262/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9958\n\nEpoch 00262: accuracy did not improve from 0.99672\nEpoch 263/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9947\n\nEpoch 00263: accuracy did not improve from 0.99672\nEpoch 264/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9939\n\nEpoch 00264: accuracy did not improve from 0.99672\nEpoch 265/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9938\n\nEpoch 00265: accuracy did not improve from 0.99672\nEpoch 266/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9946\n\nEpoch 00266: accuracy did not improve from 0.99672\nEpoch 267/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9929\n\nEpoch 00267: accuracy did not improve from 0.99672\nEpoch 268/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9948\n\nEpoch 00268: accuracy did not improve from 0.99672\nEpoch 269/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9908\n\nEpoch 00269: accuracy did not improve from 0.99672\nEpoch 270/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9913\n\nEpoch 00270: accuracy did not improve from 0.99672\nEpoch 271/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.9930\n\nEpoch 00271: accuracy did not improve from 0.99672\nEpoch 272/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9941\n\nEpoch 00272: accuracy did not improve from 0.99672\nEpoch 273/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9944\n\nEpoch 00273: accuracy did not improve from 0.99672\nEpoch 274/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9934\n\nEpoch 00274: accuracy did not improve from 0.99672\nEpoch 275/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.9942\n\nEpoch 00275: accuracy did not improve from 0.99672\nEpoch 276/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9924\n\nEpoch 00276: accuracy did not improve from 0.99672\nEpoch 277/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.9923\n\nEpoch 00277: accuracy did not improve from 0.99672\nEpoch 278/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9921\n\nEpoch 00278: accuracy did not improve from 0.99672\nEpoch 279/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9941\n\nEpoch 00279: accuracy did not improve from 0.99672\nEpoch 280/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.9923\n\nEpoch 00280: accuracy did not improve from 0.99672\nEpoch 281/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9940\n\nEpoch 00281: accuracy did not improve from 0.99672\nEpoch 282/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9958\n\nEpoch 00282: accuracy did not improve from 0.99672\nEpoch 283/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9948\n\nEpoch 00283: accuracy did not improve from 0.99672\nEpoch 284/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.9952\n\nEpoch 00284: accuracy did not improve from 0.99672\nEpoch 285/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9957\n\nEpoch 00285: accuracy did not improve from 0.99672\nEpoch 286/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9948\n\nEpoch 00286: accuracy did not improve from 0.99672\nEpoch 287/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 0.9955\n\nEpoch 00287: accuracy did not improve from 0.99672\nEpoch 288/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9942\n\nEpoch 00288: accuracy did not improve from 0.99672\nEpoch 289/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9943\n\nEpoch 00289: accuracy did not improve from 0.99672\nEpoch 290/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9953\n\nEpoch 00290: accuracy did not improve from 0.99672\nEpoch 291/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.9919\n\nEpoch 00291: accuracy did not improve from 0.99672\nEpoch 292/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9942\n\nEpoch 00292: accuracy did not improve from 0.99672\nEpoch 293/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9935\n\nEpoch 00293: accuracy did not improve from 0.99672\nEpoch 294/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9942\n\nEpoch 00294: accuracy did not improve from 0.99672\nEpoch 295/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9922\n\nEpoch 00295: accuracy did not improve from 0.99672\nEpoch 296/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.9932\n\nEpoch 00296: accuracy did not improve from 0.99672\nEpoch 297/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.9919\n\nEpoch 00297: accuracy did not improve from 0.99672\nEpoch 298/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9952\n\nEpoch 00298: accuracy did not improve from 0.99672\nEpoch 299/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9948\n\nEpoch 00299: accuracy did not improve from 0.99672\nEpoch 300/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9958\n\nEpoch 00300: accuracy did not improve from 0.99672\nEpoch 301/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9932\n\nEpoch 00301: accuracy did not improve from 0.99672\nEpoch 302/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9936\n\nEpoch 00302: accuracy did not improve from 0.99672\nEpoch 303/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 0.9946\n\nEpoch 00303: accuracy did not improve from 0.99672\nEpoch 304/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9974\n\nEpoch 00304: accuracy improved from 0.99672 to 0.99742, saving model to eq_solver.h5\nEpoch 305/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9952\n\nEpoch 00305: accuracy did not improve from 0.99742\nEpoch 306/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9957\n\nEpoch 00306: accuracy did not improve from 0.99742\nEpoch 307/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9968\n\nEpoch 00307: accuracy did not improve from 0.99742\nEpoch 308/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9958\n\nEpoch 00308: accuracy did not improve from 0.99742\nEpoch 309/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9956\n\nEpoch 00309: accuracy did not improve from 0.99742\nEpoch 310/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9943\n\nEpoch 00310: accuracy did not improve from 0.99742\nEpoch 311/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9952\n\nEpoch 00311: accuracy did not improve from 0.99742\nEpoch 312/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9958\n\nEpoch 00312: accuracy did not improve from 0.99742\nEpoch 313/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9961\n\nEpoch 00313: accuracy did not improve from 0.99742\nEpoch 314/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9957\n\nEpoch 00314: accuracy did not improve from 0.99742\nEpoch 315/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9957\n\nEpoch 00315: accuracy did not improve from 0.99742\nEpoch 316/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9942\n\nEpoch 00316: accuracy did not improve from 0.99742\nEpoch 317/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9946\n\nEpoch 00317: accuracy did not improve from 0.99742\nEpoch 318/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9938\n\nEpoch 00318: accuracy did not improve from 0.99742\nEpoch 319/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9925\n\nEpoch 00319: accuracy did not improve from 0.99742\nEpoch 320/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9942\n\nEpoch 00320: accuracy did not improve from 0.99742\nEpoch 321/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9948\n\nEpoch 00321: accuracy did not improve from 0.99742\nEpoch 322/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 0.9947\n\nEpoch 00322: accuracy did not improve from 0.99742\nEpoch 323/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9929\n\nEpoch 00323: accuracy did not improve from 0.99742\nEpoch 324/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0165 - accuracy: 0.9956\n\nEpoch 00324: accuracy did not improve from 0.99742\nEpoch 325/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9953\n\nEpoch 00325: accuracy did not improve from 0.99742\nEpoch 326/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 0.9943\n\nEpoch 00326: accuracy did not improve from 0.99742\nEpoch 327/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9952\n\nEpoch 00327: accuracy did not improve from 0.99742\nEpoch 328/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9963\n\nEpoch 00328: accuracy did not improve from 0.99742\nEpoch 329/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9949\n\nEpoch 00329: accuracy did not improve from 0.99742\nEpoch 330/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9946\n\nEpoch 00330: accuracy did not improve from 0.99742\nEpoch 331/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9950\n\nEpoch 00331: accuracy did not improve from 0.99742\nEpoch 332/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9948\n\nEpoch 00332: accuracy did not improve from 0.99742\nEpoch 333/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9944\n\nEpoch 00333: accuracy did not improve from 0.99742\nEpoch 334/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9941\n\nEpoch 00334: accuracy did not improve from 0.99742\nEpoch 335/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9930\n\nEpoch 00335: accuracy did not improve from 0.99742\nEpoch 336/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9962\n\nEpoch 00336: accuracy did not improve from 0.99742\nEpoch 337/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9952\n\nEpoch 00337: accuracy did not improve from 0.99742\nEpoch 338/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9950\n\nEpoch 00338: accuracy did not improve from 0.99742\nEpoch 339/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9955\n\nEpoch 00339: accuracy did not improve from 0.99742\nEpoch 340/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9959\n\nEpoch 00340: accuracy did not improve from 0.99742\nEpoch 341/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9965\n\nEpoch 00341: accuracy did not improve from 0.99742\nEpoch 342/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9968\n\nEpoch 00342: accuracy did not improve from 0.99742\nEpoch 343/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.9938\n\nEpoch 00343: accuracy did not improve from 0.99742\nEpoch 344/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9932\n\nEpoch 00344: accuracy did not improve from 0.99742\nEpoch 345/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.9947\n\nEpoch 00345: accuracy did not improve from 0.99742\nEpoch 346/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9949\n\nEpoch 00346: accuracy did not improve from 0.99742\nEpoch 347/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9945\n\nEpoch 00347: accuracy did not improve from 0.99742\nEpoch 348/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.9942\n\nEpoch 00348: accuracy did not improve from 0.99742\nEpoch 349/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9956\n\nEpoch 00349: accuracy did not improve from 0.99742\nEpoch 350/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.9942\n\nEpoch 00350: accuracy did not improve from 0.99742\nEpoch 351/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9947\n\nEpoch 00351: accuracy did not improve from 0.99742\nEpoch 352/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9960\n\nEpoch 00352: accuracy did not improve from 0.99742\nEpoch 353/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.9945\n\nEpoch 00353: accuracy did not improve from 0.99742\nEpoch 354/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9955\n\nEpoch 00354: accuracy did not improve from 0.99742\nEpoch 355/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9962\n\nEpoch 00355: accuracy did not improve from 0.99742\nEpoch 356/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9972\n\nEpoch 00356: accuracy did not improve from 0.99742\nEpoch 357/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9955\n\nEpoch 00357: accuracy did not improve from 0.99742\nEpoch 358/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9965\n\nEpoch 00358: accuracy did not improve from 0.99742\nEpoch 359/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9965\n\nEpoch 00359: accuracy did not improve from 0.99742\nEpoch 360/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9969\n\nEpoch 00360: accuracy did not improve from 0.99742\nEpoch 361/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9950\n\nEpoch 00361: accuracy did not improve from 0.99742\nEpoch 362/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9957\n\nEpoch 00362: accuracy did not improve from 0.99742\nEpoch 363/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9951\n\nEpoch 00363: accuracy did not improve from 0.99742\nEpoch 364/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9947\n\nEpoch 00364: accuracy did not improve from 0.99742\nEpoch 365/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9948\n\nEpoch 00365: accuracy did not improve from 0.99742\nEpoch 366/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9951\n\nEpoch 00366: accuracy did not improve from 0.99742\nEpoch 367/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9948\n\nEpoch 00367: accuracy did not improve from 0.99742\nEpoch 368/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9949\n\nEpoch 00368: accuracy did not improve from 0.99742\nEpoch 369/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9934\n\nEpoch 00369: accuracy did not improve from 0.99742\nEpoch 370/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.9930\n\nEpoch 00370: accuracy did not improve from 0.99742\nEpoch 371/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9942\n\nEpoch 00371: accuracy did not improve from 0.99742\nEpoch 372/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9950\n\nEpoch 00372: accuracy did not improve from 0.99742\nEpoch 373/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9930\n\nEpoch 00373: accuracy did not improve from 0.99742\nEpoch 374/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.9935\n\nEpoch 00374: accuracy did not improve from 0.99742\nEpoch 375/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9960\n\nEpoch 00375: accuracy did not improve from 0.99742\nEpoch 376/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9962\n\nEpoch 00376: accuracy did not improve from 0.99742\nEpoch 377/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 0.9959\n\nEpoch 00377: accuracy did not improve from 0.99742\nEpoch 378/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9961\n\nEpoch 00378: accuracy did not improve from 0.99742\nEpoch 379/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9960\n\nEpoch 00379: accuracy did not improve from 0.99742\nEpoch 380/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9954\n\nEpoch 00380: accuracy did not improve from 0.99742\nEpoch 381/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9966\n\nEpoch 00381: accuracy did not improve from 0.99742\nEpoch 382/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0147 - accuracy: 0.9949\n\nEpoch 00382: accuracy did not improve from 0.99742\nEpoch 383/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9971\n\nEpoch 00383: accuracy did not improve from 0.99742\nEpoch 384/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9953\n\nEpoch 00384: accuracy did not improve from 0.99742\nEpoch 385/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9963\n\nEpoch 00385: accuracy did not improve from 0.99742\nEpoch 386/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 0.9963\n\nEpoch 00386: accuracy did not improve from 0.99742\nEpoch 387/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9959\n\nEpoch 00387: accuracy did not improve from 0.99742\nEpoch 388/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9958\n\nEpoch 00388: accuracy did not improve from 0.99742\nEpoch 389/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9950\n\nEpoch 00389: accuracy did not improve from 0.99742\nEpoch 390/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.9946\n\nEpoch 00390: accuracy did not improve from 0.99742\nEpoch 391/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9951\n\nEpoch 00391: accuracy did not improve from 0.99742\nEpoch 392/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9965\n\nEpoch 00392: accuracy did not improve from 0.99742\nEpoch 393/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9958\n\nEpoch 00393: accuracy did not improve from 0.99742\nEpoch 394/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.9947\n\nEpoch 00394: accuracy did not improve from 0.99742\nEpoch 395/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9951\n\nEpoch 00395: accuracy did not improve from 0.99742\nEpoch 396/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9966\n\nEpoch 00396: accuracy did not improve from 0.99742\nEpoch 397/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9964\n\nEpoch 00397: accuracy did not improve from 0.99742\nEpoch 398/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 0.9956\n\nEpoch 00398: accuracy did not improve from 0.99742\nEpoch 399/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9957\n\nEpoch 00399: accuracy did not improve from 0.99742\nEpoch 400/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.9957\n\nEpoch 00400: accuracy did not improve from 0.99742\nEpoch 401/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9964\n\nEpoch 00401: accuracy did not improve from 0.99742\nEpoch 402/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0145 - accuracy: 0.9958\n\nEpoch 00402: accuracy did not improve from 0.99742\nEpoch 403/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 0.9953\n\nEpoch 00403: accuracy did not improve from 0.99742\nEpoch 404/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9971\n\nEpoch 00404: accuracy did not improve from 0.99742\nEpoch 405/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9963\n\nEpoch 00405: accuracy did not improve from 0.99742\nEpoch 406/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9954\n\nEpoch 00406: accuracy did not improve from 0.99742\nEpoch 407/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9956\n\nEpoch 00407: accuracy did not improve from 0.99742\nEpoch 408/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9972\n\nEpoch 00408: accuracy did not improve from 0.99742\nEpoch 409/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9965\n\nEpoch 00409: accuracy did not improve from 0.99742\nEpoch 410/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9951\n\nEpoch 00410: accuracy did not improve from 0.99742\nEpoch 411/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9956\n\nEpoch 00411: accuracy did not improve from 0.99742\nEpoch 412/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9962\n\nEpoch 00412: accuracy did not improve from 0.99742\nEpoch 413/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9964\n\nEpoch 00413: accuracy did not improve from 0.99742\nEpoch 414/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9978\n\nEpoch 00414: accuracy improved from 0.99742 to 0.99782, saving model to eq_solver.h5\nEpoch 415/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9970\n\nEpoch 00415: accuracy did not improve from 0.99782\nEpoch 416/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.9964\n\nEpoch 00416: accuracy did not improve from 0.99782\nEpoch 417/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9962\n\nEpoch 00417: accuracy did not improve from 0.99782\nEpoch 418/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9942\n\nEpoch 00418: accuracy did not improve from 0.99782\nEpoch 419/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9946\n\nEpoch 00419: accuracy did not improve from 0.99782\nEpoch 420/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 0.9954\n\nEpoch 00420: accuracy did not improve from 0.99782\nEpoch 421/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9937\n\nEpoch 00421: accuracy did not improve from 0.99782\nEpoch 422/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9959\n\nEpoch 00422: accuracy did not improve from 0.99782\nEpoch 423/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9961\n\nEpoch 00423: accuracy did not improve from 0.99782\nEpoch 424/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9956\n\nEpoch 00424: accuracy did not improve from 0.99782\nEpoch 425/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9963\n\nEpoch 00425: accuracy did not improve from 0.99782\nEpoch 426/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9951\n\nEpoch 00426: accuracy did not improve from 0.99782\nEpoch 427/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9950\n\nEpoch 00427: accuracy did not improve from 0.99782\nEpoch 428/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9957\n\nEpoch 00428: accuracy did not improve from 0.99782\nEpoch 429/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.9950\n\nEpoch 00429: accuracy did not improve from 0.99782\nEpoch 430/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9956\n\nEpoch 00430: accuracy did not improve from 0.99782\nEpoch 431/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9962\n\nEpoch 00431: accuracy did not improve from 0.99782\nEpoch 432/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9973\n\nEpoch 00432: accuracy did not improve from 0.99782\nEpoch 433/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9967\n\nEpoch 00433: accuracy did not improve from 0.99782\nEpoch 434/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9964\n\nEpoch 00434: accuracy did not improve from 0.99782\nEpoch 435/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9968\n\nEpoch 00435: accuracy did not improve from 0.99782\nEpoch 436/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9954\n\nEpoch 00436: accuracy did not improve from 0.99782\nEpoch 437/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0162 - accuracy: 0.9954\n\nEpoch 00437: accuracy did not improve from 0.99782\nEpoch 438/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9965\n\nEpoch 00438: accuracy did not improve from 0.99782\nEpoch 439/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 0.9958\n\nEpoch 00439: accuracy did not improve from 0.99782\nEpoch 440/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9969\n\nEpoch 00440: accuracy did not improve from 0.99782\nEpoch 441/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.9951\n\nEpoch 00441: accuracy did not improve from 0.99782\nEpoch 442/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9952\n\nEpoch 00442: accuracy did not improve from 0.99782\nEpoch 443/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9958\n\nEpoch 00443: accuracy did not improve from 0.99782\nEpoch 444/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9953\n\nEpoch 00444: accuracy did not improve from 0.99782\nEpoch 445/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9958\n\nEpoch 00445: accuracy did not improve from 0.99782\nEpoch 446/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9960\n\nEpoch 00446: accuracy did not improve from 0.99782\nEpoch 447/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9934\n\nEpoch 00447: accuracy did not improve from 0.99782\nEpoch 448/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9964\n\nEpoch 00448: accuracy did not improve from 0.99782\nEpoch 449/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9957\n\nEpoch 00449: accuracy did not improve from 0.99782\nEpoch 450/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9959\n\nEpoch 00450: accuracy did not improve from 0.99782\nEpoch 451/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9959\n\nEpoch 00451: accuracy did not improve from 0.99782\nEpoch 452/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.9954\n\nEpoch 00452: accuracy did not improve from 0.99782\nEpoch 453/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9949\n\nEpoch 00453: accuracy did not improve from 0.99782\nEpoch 454/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9950\n\nEpoch 00454: accuracy did not improve from 0.99782\nEpoch 455/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9959\n\nEpoch 00455: accuracy did not improve from 0.99782\nEpoch 456/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9960\n\nEpoch 00456: accuracy did not improve from 0.99782\nEpoch 457/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9965\n\nEpoch 00457: accuracy did not improve from 0.99782\nEpoch 458/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9962\n\nEpoch 00458: accuracy did not improve from 0.99782\nEpoch 459/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9968\n\nEpoch 00459: accuracy did not improve from 0.99782\nEpoch 460/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9968\n\nEpoch 00460: accuracy did not improve from 0.99782\nEpoch 461/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9968\n\nEpoch 00461: accuracy did not improve from 0.99782\nEpoch 462/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9972\n\nEpoch 00462: accuracy did not improve from 0.99782\nEpoch 463/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.9979\n\nEpoch 00463: accuracy improved from 0.99782 to 0.99791, saving model to eq_solver.h5\nEpoch 464/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9967\n\nEpoch 00464: accuracy did not improve from 0.99791\nEpoch 465/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9975\n\nEpoch 00465: accuracy did not improve from 0.99791\nEpoch 466/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9973\n\nEpoch 00466: accuracy did not improve from 0.99791\nEpoch 467/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9976\n\nEpoch 00467: accuracy did not improve from 0.99791\nEpoch 468/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9965\n\nEpoch 00468: accuracy did not improve from 0.99791\nEpoch 469/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 0.9963\n\nEpoch 00469: accuracy did not improve from 0.99791\nEpoch 470/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9966\n\nEpoch 00470: accuracy did not improve from 0.99791\nEpoch 471/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9972\n\nEpoch 00471: accuracy did not improve from 0.99791\nEpoch 472/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9968\n\nEpoch 00472: accuracy did not improve from 0.99791\nEpoch 473/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9963\n\nEpoch 00473: accuracy did not improve from 0.99791\nEpoch 474/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9960\n\nEpoch 00474: accuracy did not improve from 0.99791\nEpoch 475/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9962\n\nEpoch 00475: accuracy did not improve from 0.99791\nEpoch 476/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9967\n\nEpoch 00476: accuracy did not improve from 0.99791\nEpoch 477/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9961\n\nEpoch 00477: accuracy did not improve from 0.99791\nEpoch 478/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9974\n\nEpoch 00478: accuracy did not improve from 0.99791\nEpoch 479/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9950\n\nEpoch 00479: accuracy did not improve from 0.99791\nEpoch 480/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9966\n\nEpoch 00480: accuracy did not improve from 0.99791\nEpoch 481/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9973\n\nEpoch 00481: accuracy did not improve from 0.99791\nEpoch 482/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9968\n\nEpoch 00482: accuracy did not improve from 0.99791\nEpoch 483/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9965\n\nEpoch 00483: accuracy did not improve from 0.99791\nEpoch 484/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.9950\n\nEpoch 00484: accuracy did not improve from 0.99791\nEpoch 485/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9950\n\nEpoch 00485: accuracy did not improve from 0.99791\nEpoch 486/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9964\n\nEpoch 00486: accuracy did not improve from 0.99791\nEpoch 487/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9969\n\nEpoch 00487: accuracy did not improve from 0.99791\nEpoch 488/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9965\n\nEpoch 00488: accuracy did not improve from 0.99791\nEpoch 489/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9964\n\nEpoch 00489: accuracy did not improve from 0.99791\nEpoch 490/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9975\n\nEpoch 00490: accuracy did not improve from 0.99791\nEpoch 491/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9968\n\nEpoch 00491: accuracy did not improve from 0.99791\nEpoch 492/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9973\n\nEpoch 00492: accuracy did not improve from 0.99791\nEpoch 493/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 0.9964\n\nEpoch 00493: accuracy did not improve from 0.99791\nEpoch 494/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9955\n\nEpoch 00494: accuracy did not improve from 0.99791\nEpoch 495/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9963\n\nEpoch 00495: accuracy did not improve from 0.99791\nEpoch 496/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9957\n\nEpoch 00496: accuracy did not improve from 0.99791\nEpoch 497/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9945\n\nEpoch 00497: accuracy did not improve from 0.99791\nEpoch 498/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9960\n\nEpoch 00498: accuracy did not improve from 0.99791\nEpoch 499/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9972\n\nEpoch 00499: accuracy did not improve from 0.99791\nEpoch 500/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9950\n\nEpoch 00500: accuracy did not improve from 0.99791\nEpoch 501/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9954\n\nEpoch 00501: accuracy did not improve from 0.99791\nEpoch 502/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9970\n\nEpoch 00502: accuracy did not improve from 0.99791\nEpoch 503/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9972\n\nEpoch 00503: accuracy did not improve from 0.99791\nEpoch 504/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9971\n\nEpoch 00504: accuracy did not improve from 0.99791\nEpoch 505/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9970\n\nEpoch 00505: accuracy did not improve from 0.99791\nEpoch 506/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9974\n\nEpoch 00506: accuracy did not improve from 0.99791\nEpoch 507/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9972\n\nEpoch 00507: accuracy did not improve from 0.99791\nEpoch 508/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9972\n\nEpoch 00508: accuracy did not improve from 0.99791\nEpoch 509/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0200 - accuracy: 0.9946\n\nEpoch 00509: accuracy did not improve from 0.99791\nEpoch 510/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 0.9959\n\nEpoch 00510: accuracy did not improve from 0.99791\nEpoch 511/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9952\n\nEpoch 00511: accuracy did not improve from 0.99791\nEpoch 512/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9966\n\nEpoch 00512: accuracy did not improve from 0.99791\nEpoch 513/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9976\n\nEpoch 00513: accuracy did not improve from 0.99791\nEpoch 514/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9949\n\nEpoch 00514: accuracy did not improve from 0.99791\nEpoch 515/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9966\n\nEpoch 00515: accuracy did not improve from 0.99791\nEpoch 516/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9969\n\nEpoch 00516: accuracy did not improve from 0.99791\nEpoch 517/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.9962\n\nEpoch 00517: accuracy did not improve from 0.99791\nEpoch 518/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0070 - accuracy: 0.9975\n\nEpoch 00518: accuracy did not improve from 0.99791\nEpoch 519/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9981\n\nEpoch 00519: accuracy improved from 0.99791 to 0.99811, saving model to eq_solver.h5\nEpoch 520/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9980\n\nEpoch 00520: accuracy did not improve from 0.99811\nEpoch 521/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9975\n\nEpoch 00521: accuracy did not improve from 0.99811\nEpoch 522/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9970\n\nEpoch 00522: accuracy did not improve from 0.99811\nEpoch 523/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9967\n\nEpoch 00523: accuracy did not improve from 0.99811\nEpoch 524/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9954\n\nEpoch 00524: accuracy did not improve from 0.99811\nEpoch 525/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9958\n\nEpoch 00525: accuracy did not improve from 0.99811\nEpoch 526/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9962\n\nEpoch 00526: accuracy did not improve from 0.99811\nEpoch 527/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9958\n\nEpoch 00527: accuracy did not improve from 0.99811\nEpoch 528/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9972\n\nEpoch 00528: accuracy did not improve from 0.99811\nEpoch 529/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9965\n\nEpoch 00529: accuracy did not improve from 0.99811\nEpoch 530/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9975\n\nEpoch 00530: accuracy did not improve from 0.99811\nEpoch 531/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 0.9957\n\nEpoch 00531: accuracy did not improve from 0.99811\nEpoch 532/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.9957\n\nEpoch 00532: accuracy did not improve from 0.99811\nEpoch 533/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9963\n\nEpoch 00533: accuracy did not improve from 0.99811\nEpoch 534/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9974\n\nEpoch 00534: accuracy did not improve from 0.99811\nEpoch 535/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9974\n\nEpoch 00535: accuracy did not improve from 0.99811\nEpoch 536/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9974\n\nEpoch 00536: accuracy did not improve from 0.99811\nEpoch 537/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9980\n\nEpoch 00537: accuracy did not improve from 0.99811\nEpoch 538/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9969\n\nEpoch 00538: accuracy did not improve from 0.99811\nEpoch 539/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9968\n\nEpoch 00539: accuracy did not improve from 0.99811\nEpoch 540/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9968\n\nEpoch 00540: accuracy did not improve from 0.99811\nEpoch 541/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9970\n\nEpoch 00541: accuracy did not improve from 0.99811\nEpoch 542/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9953\n\nEpoch 00542: accuracy did not improve from 0.99811\nEpoch 543/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9954\n\nEpoch 00543: accuracy did not improve from 0.99811\nEpoch 544/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9964\n\nEpoch 00544: accuracy did not improve from 0.99811\nEpoch 545/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9965\n\nEpoch 00545: accuracy did not improve from 0.99811\nEpoch 546/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9958\n\nEpoch 00546: accuracy did not improve from 0.99811\nEpoch 547/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9968\n\nEpoch 00547: accuracy did not improve from 0.99811\nEpoch 548/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9969\n\nEpoch 00548: accuracy did not improve from 0.99811\nEpoch 549/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9963\n\nEpoch 00549: accuracy did not improve from 0.99811\nEpoch 550/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9952\n\nEpoch 00550: accuracy did not improve from 0.99811\nEpoch 551/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9934\n\nEpoch 00551: accuracy did not improve from 0.99811\nEpoch 552/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9947\n\nEpoch 00552: accuracy did not improve from 0.99811\nEpoch 553/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9963\n\nEpoch 00553: accuracy did not improve from 0.99811\nEpoch 554/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0165 - accuracy: 0.9967\n\nEpoch 00554: accuracy did not improve from 0.99811\nEpoch 555/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9966\n\nEpoch 00555: accuracy did not improve from 0.99811\nEpoch 556/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 0.9963\n\nEpoch 00556: accuracy did not improve from 0.99811\nEpoch 557/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9972\n\nEpoch 00557: accuracy did not improve from 0.99811\nEpoch 558/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9973\n\nEpoch 00558: accuracy did not improve from 0.99811\nEpoch 559/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9970\n\nEpoch 00559: accuracy did not improve from 0.99811\nEpoch 560/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9948\n\nEpoch 00560: accuracy did not improve from 0.99811\nEpoch 561/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9961\n\nEpoch 00561: accuracy did not improve from 0.99811\nEpoch 562/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9971\n\nEpoch 00562: accuracy did not improve from 0.99811\nEpoch 563/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9980\n\nEpoch 00563: accuracy did not improve from 0.99811\nEpoch 564/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9967\n\nEpoch 00564: accuracy did not improve from 0.99811\nEpoch 565/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9974\n\nEpoch 00565: accuracy did not improve from 0.99811\nEpoch 566/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9981\n\nEpoch 00566: accuracy did not improve from 0.99811\nEpoch 567/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9972\n\nEpoch 00567: accuracy did not improve from 0.99811\nEpoch 568/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9974\n\nEpoch 00568: accuracy did not improve from 0.99811\nEpoch 569/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9971\n\nEpoch 00569: accuracy did not improve from 0.99811\nEpoch 570/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9977\n\nEpoch 00570: accuracy did not improve from 0.99811\nEpoch 571/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9972\n\nEpoch 00571: accuracy did not improve from 0.99811\nEpoch 572/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9977\n\nEpoch 00572: accuracy did not improve from 0.99811\nEpoch 573/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 0.9975\n\nEpoch 00573: accuracy did not improve from 0.99811\nEpoch 574/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 0.9974\n\nEpoch 00574: accuracy did not improve from 0.99811\nEpoch 575/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 0.9966\n\nEpoch 00575: accuracy did not improve from 0.99811\nEpoch 576/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9970\n\nEpoch 00576: accuracy did not improve from 0.99811\nEpoch 577/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9961\n\nEpoch 00577: accuracy did not improve from 0.99811\nEpoch 578/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9969\n\nEpoch 00578: accuracy did not improve from 0.99811\nEpoch 579/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 0.9956\n\nEpoch 00579: accuracy did not improve from 0.99811\nEpoch 580/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9972\n\nEpoch 00580: accuracy did not improve from 0.99811\nEpoch 581/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.9961\n\nEpoch 00581: accuracy did not improve from 0.99811\nEpoch 582/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9963\n\nEpoch 00582: accuracy did not improve from 0.99811\nEpoch 583/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9970\n\nEpoch 00583: accuracy did not improve from 0.99811\nEpoch 584/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9971\n\nEpoch 00584: accuracy did not improve from 0.99811\nEpoch 585/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9953\n\nEpoch 00585: accuracy did not improve from 0.99811\nEpoch 586/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9960\n\nEpoch 00586: accuracy did not improve from 0.99811\nEpoch 587/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9968\n\nEpoch 00587: accuracy did not improve from 0.99811\nEpoch 588/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9969\n\nEpoch 00588: accuracy did not improve from 0.99811\nEpoch 589/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9966\n\nEpoch 00589: accuracy did not improve from 0.99811\nEpoch 590/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9961\n\nEpoch 00590: accuracy did not improve from 0.99811\nEpoch 591/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9969\n\nEpoch 00591: accuracy did not improve from 0.99811\nEpoch 592/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9968\n\nEpoch 00592: accuracy did not improve from 0.99811\nEpoch 593/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9959\n\nEpoch 00593: accuracy did not improve from 0.99811\nEpoch 594/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9959\n\nEpoch 00594: accuracy did not improve from 0.99811\nEpoch 595/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9953\n\nEpoch 00595: accuracy did not improve from 0.99811\nEpoch 596/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 0.9960\n\nEpoch 00596: accuracy did not improve from 0.99811\nEpoch 597/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9944\n\nEpoch 00597: accuracy did not improve from 0.99811\nEpoch 598/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9968\n\nEpoch 00598: accuracy did not improve from 0.99811\nEpoch 599/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 0.9981\n\nEpoch 00599: accuracy did not improve from 0.99811\nEpoch 600/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9957\n\nEpoch 00600: accuracy did not improve from 0.99811\nEpoch 601/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9960\n\nEpoch 00601: accuracy did not improve from 0.99811\nEpoch 602/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9970\n\nEpoch 00602: accuracy did not improve from 0.99811\nEpoch 603/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9966\n\nEpoch 00603: accuracy did not improve from 0.99811\nEpoch 604/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9955\n\nEpoch 00604: accuracy did not improve from 0.99811\nEpoch 605/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9964\n\nEpoch 00605: accuracy did not improve from 0.99811\nEpoch 606/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9967\n\nEpoch 00606: accuracy did not improve from 0.99811\nEpoch 607/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9965\n\nEpoch 00607: accuracy did not improve from 0.99811\nEpoch 608/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9964\n\nEpoch 00608: accuracy did not improve from 0.99811\nEpoch 609/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.9975\n\nEpoch 00609: accuracy did not improve from 0.99811\nEpoch 610/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9972\n\nEpoch 00610: accuracy did not improve from 0.99811\nEpoch 611/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9972\n\nEpoch 00611: accuracy did not improve from 0.99811\nEpoch 612/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9962\n\nEpoch 00612: accuracy did not improve from 0.99811\nEpoch 613/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9975\n\nEpoch 00613: accuracy did not improve from 0.99811\nEpoch 614/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.9984\n\nEpoch 00614: accuracy improved from 0.99811 to 0.99841, saving model to eq_solver.h5\nEpoch 615/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9973\n\nEpoch 00615: accuracy did not improve from 0.99841\nEpoch 616/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9972\n\nEpoch 00616: accuracy did not improve from 0.99841\nEpoch 617/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9970\n\nEpoch 00617: accuracy did not improve from 0.99841\nEpoch 618/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9977\n\nEpoch 00618: accuracy did not improve from 0.99841\nEpoch 619/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9966\n\nEpoch 00619: accuracy did not improve from 0.99841\nEpoch 620/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9975\n\nEpoch 00620: accuracy did not improve from 0.99841\nEpoch 621/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9971\n\nEpoch 00621: accuracy did not improve from 0.99841\nEpoch 622/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.9972\n\nEpoch 00622: accuracy did not improve from 0.99841\nEpoch 623/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9968\n\nEpoch 00623: accuracy did not improve from 0.99841\nEpoch 624/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0171 - accuracy: 0.9960\n\nEpoch 00624: accuracy did not improve from 0.99841\nEpoch 625/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9965\n\nEpoch 00625: accuracy did not improve from 0.99841\nEpoch 626/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9969\n\nEpoch 00626: accuracy did not improve from 0.99841\nEpoch 627/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9978\n\nEpoch 00627: accuracy did not improve from 0.99841\nEpoch 628/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9973\n\nEpoch 00628: accuracy did not improve from 0.99841\nEpoch 629/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.9977\n\nEpoch 00629: accuracy did not improve from 0.99841\nEpoch 630/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9966\n\nEpoch 00630: accuracy did not improve from 0.99841\nEpoch 631/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9961\n\nEpoch 00631: accuracy did not improve from 0.99841\nEpoch 632/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9968\n\nEpoch 00632: accuracy did not improve from 0.99841\nEpoch 633/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.9976\n\nEpoch 00633: accuracy did not improve from 0.99841\nEpoch 634/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9969\n\nEpoch 00634: accuracy did not improve from 0.99841\nEpoch 635/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9975\n\nEpoch 00635: accuracy did not improve from 0.99841\nEpoch 636/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9970\n\nEpoch 00636: accuracy did not improve from 0.99841\nEpoch 637/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9970\n\nEpoch 00637: accuracy did not improve from 0.99841\nEpoch 638/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9972\n\nEpoch 00638: accuracy did not improve from 0.99841\nEpoch 639/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9970\n\nEpoch 00639: accuracy did not improve from 0.99841\nEpoch 640/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9978\n\nEpoch 00640: accuracy did not improve from 0.99841\nEpoch 641/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9974\n\nEpoch 00641: accuracy did not improve from 0.99841\nEpoch 642/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9977\n\nEpoch 00642: accuracy did not improve from 0.99841\nEpoch 643/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9977\n\nEpoch 00643: accuracy did not improve from 0.99841\nEpoch 644/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9977\n\nEpoch 00644: accuracy did not improve from 0.99841\nEpoch 645/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9973\n\nEpoch 00645: accuracy did not improve from 0.99841\nEpoch 646/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9967\n\nEpoch 00646: accuracy did not improve from 0.99841\nEpoch 647/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9970\n\nEpoch 00647: accuracy did not improve from 0.99841\nEpoch 648/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9961\n\nEpoch 00648: accuracy did not improve from 0.99841\nEpoch 649/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9969\n\nEpoch 00649: accuracy did not improve from 0.99841\nEpoch 650/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9968\n\nEpoch 00650: accuracy did not improve from 0.99841\nEpoch 651/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9971\n\nEpoch 00651: accuracy did not improve from 0.99841\nEpoch 652/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9981\n\nEpoch 00652: accuracy did not improve from 0.99841\nEpoch 653/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9974\n\nEpoch 00653: accuracy did not improve from 0.99841\nEpoch 654/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9976\n\nEpoch 00654: accuracy did not improve from 0.99841\nEpoch 655/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.9985\n\nEpoch 00655: accuracy improved from 0.99841 to 0.99851, saving model to eq_solver.h5\nEpoch 656/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 0.9978\n\nEpoch 00656: accuracy did not improve from 0.99851\nEpoch 657/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9978\n\nEpoch 00657: accuracy did not improve from 0.99851\nEpoch 658/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.9984\n\nEpoch 00658: accuracy did not improve from 0.99851\nEpoch 659/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9982\n\nEpoch 00659: accuracy did not improve from 0.99851\nEpoch 660/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9975\n\nEpoch 00660: accuracy did not improve from 0.99851\nEpoch 661/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9963\n\nEpoch 00661: accuracy did not improve from 0.99851\nEpoch 662/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9969\n\nEpoch 00662: accuracy did not improve from 0.99851\nEpoch 663/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9966\n\nEpoch 00663: accuracy did not improve from 0.99851\nEpoch 664/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9965\n\nEpoch 00664: accuracy did not improve from 0.99851\nEpoch 665/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9961\n\nEpoch 00665: accuracy did not improve from 0.99851\nEpoch 666/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.9947\n\nEpoch 00666: accuracy did not improve from 0.99851\nEpoch 667/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9957\n\nEpoch 00667: accuracy did not improve from 0.99851\nEpoch 668/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9961\n\nEpoch 00668: accuracy did not improve from 0.99851\nEpoch 669/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.9983\n\nEpoch 00669: accuracy did not improve from 0.99851\nEpoch 670/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9970\n\nEpoch 00670: accuracy did not improve from 0.99851\nEpoch 671/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9981\n\nEpoch 00671: accuracy did not improve from 0.99851\nEpoch 672/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9985\n\nEpoch 00672: accuracy did not improve from 0.99851\nEpoch 673/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9973\n\nEpoch 00673: accuracy did not improve from 0.99851\nEpoch 674/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9971\n\nEpoch 00674: accuracy did not improve from 0.99851\nEpoch 675/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9966\n\nEpoch 00675: accuracy did not improve from 0.99851\nEpoch 676/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9965\n\nEpoch 00676: accuracy did not improve from 0.99851\nEpoch 677/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9966\n\nEpoch 00677: accuracy did not improve from 0.99851\nEpoch 678/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9974\n\nEpoch 00678: accuracy did not improve from 0.99851\nEpoch 679/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9976\n\nEpoch 00679: accuracy did not improve from 0.99851\nEpoch 680/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9971\n\nEpoch 00680: accuracy did not improve from 0.99851\nEpoch 681/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9972\n\nEpoch 00681: accuracy did not improve from 0.99851\nEpoch 682/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9970\n\nEpoch 00682: accuracy did not improve from 0.99851\nEpoch 683/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9977\n\nEpoch 00683: accuracy did not improve from 0.99851\nEpoch 684/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9970\n\nEpoch 00684: accuracy did not improve from 0.99851\nEpoch 685/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9970\n\nEpoch 00685: accuracy did not improve from 0.99851\nEpoch 686/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9977\n\nEpoch 00686: accuracy did not improve from 0.99851\nEpoch 687/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9968\n\nEpoch 00687: accuracy did not improve from 0.99851\nEpoch 688/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9969\n\nEpoch 00688: accuracy did not improve from 0.99851\nEpoch 689/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9975\n\nEpoch 00689: accuracy did not improve from 0.99851\nEpoch 690/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9973\n\nEpoch 00690: accuracy did not improve from 0.99851\nEpoch 691/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9969\n\nEpoch 00691: accuracy did not improve from 0.99851\nEpoch 692/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9967\n\nEpoch 00692: accuracy did not improve from 0.99851\nEpoch 693/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9971\n\nEpoch 00693: accuracy did not improve from 0.99851\nEpoch 694/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9965\n\nEpoch 00694: accuracy did not improve from 0.99851\nEpoch 695/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9966\n\nEpoch 00695: accuracy did not improve from 0.99851\nEpoch 696/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9970\n\nEpoch 00696: accuracy did not improve from 0.99851\nEpoch 697/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9972\n\nEpoch 00697: accuracy did not improve from 0.99851\nEpoch 698/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9972\n\nEpoch 00698: accuracy did not improve from 0.99851\nEpoch 699/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9975\n\nEpoch 00699: accuracy did not improve from 0.99851\nEpoch 700/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.9975\n\nEpoch 00700: accuracy did not improve from 0.99851\nEpoch 701/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.9979\n\nEpoch 00701: accuracy did not improve from 0.99851\nEpoch 702/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.9977\n\nEpoch 00702: accuracy did not improve from 0.99851\nEpoch 703/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9971\n\nEpoch 00703: accuracy did not improve from 0.99851\nEpoch 704/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9970\n\nEpoch 00704: accuracy did not improve from 0.99851\nEpoch 705/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9971\n\nEpoch 00705: accuracy did not improve from 0.99851\nEpoch 706/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9976\n\nEpoch 00706: accuracy did not improve from 0.99851\nEpoch 707/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9970\n\nEpoch 00707: accuracy did not improve from 0.99851\nEpoch 708/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9973\n\nEpoch 00708: accuracy did not improve from 0.99851\nEpoch 709/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.9976\n\nEpoch 00709: accuracy did not improve from 0.99851\nEpoch 710/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9970\n\nEpoch 00710: accuracy did not improve from 0.99851\nEpoch 711/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9965\n\nEpoch 00711: accuracy did not improve from 0.99851\nEpoch 712/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9973\n\nEpoch 00712: accuracy did not improve from 0.99851\nEpoch 713/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9971\n\nEpoch 00713: accuracy did not improve from 0.99851\nEpoch 714/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9976\n\nEpoch 00714: accuracy did not improve from 0.99851\nEpoch 715/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9977\n\nEpoch 00715: accuracy did not improve from 0.99851\nEpoch 716/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9973\n\nEpoch 00716: accuracy did not improve from 0.99851\nEpoch 717/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9966\n\nEpoch 00717: accuracy did not improve from 0.99851\nEpoch 718/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9978\n\nEpoch 00718: accuracy did not improve from 0.99851\nEpoch 719/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9981\n\nEpoch 00719: accuracy did not improve from 0.99851\nEpoch 720/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9973\n\nEpoch 00720: accuracy did not improve from 0.99851\nEpoch 721/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9964\n\nEpoch 00721: accuracy did not improve from 0.99851\nEpoch 722/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9975\n\nEpoch 00722: accuracy did not improve from 0.99851\nEpoch 723/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.9974\n\nEpoch 00723: accuracy did not improve from 0.99851\nEpoch 724/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9970\n\nEpoch 00724: accuracy did not improve from 0.99851\nEpoch 725/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.9976\n\nEpoch 00725: accuracy did not improve from 0.99851\nEpoch 726/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9980\n\nEpoch 00726: accuracy did not improve from 0.99851\nEpoch 727/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.9970\n\nEpoch 00727: accuracy did not improve from 0.99851\nEpoch 728/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9980\n\nEpoch 00728: accuracy did not improve from 0.99851\nEpoch 729/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9972\n\nEpoch 00729: accuracy did not improve from 0.99851\nEpoch 730/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9977\n\nEpoch 00730: accuracy did not improve from 0.99851\nEpoch 731/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9973\n\nEpoch 00731: accuracy did not improve from 0.99851\nEpoch 732/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9982\n\nEpoch 00732: accuracy did not improve from 0.99851\nEpoch 733/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9967\n\nEpoch 00733: accuracy did not improve from 0.99851\nEpoch 734/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9976\n\nEpoch 00734: accuracy did not improve from 0.99851\nEpoch 735/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9963\n\nEpoch 00735: accuracy did not improve from 0.99851\nEpoch 736/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9967\n\nEpoch 00736: accuracy did not improve from 0.99851\nEpoch 737/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9965\n\nEpoch 00737: accuracy did not improve from 0.99851\nEpoch 738/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9967\n\nEpoch 00738: accuracy did not improve from 0.99851\nEpoch 739/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9970\n\nEpoch 00739: accuracy did not improve from 0.99851\nEpoch 740/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9975\n\nEpoch 00740: accuracy did not improve from 0.99851\nEpoch 741/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9978\n\nEpoch 00741: accuracy did not improve from 0.99851\nEpoch 742/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9969\n\nEpoch 00742: accuracy did not improve from 0.99851\nEpoch 743/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9963\n\nEpoch 00743: accuracy did not improve from 0.99851\nEpoch 744/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9951\n\nEpoch 00744: accuracy did not improve from 0.99851\nEpoch 745/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9968\n\nEpoch 00745: accuracy did not improve from 0.99851\nEpoch 746/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.9974\n\nEpoch 00746: accuracy did not improve from 0.99851\nEpoch 747/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 0.9950\n\nEpoch 00747: accuracy did not improve from 0.99851\nEpoch 748/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.9963\n\nEpoch 00748: accuracy did not improve from 0.99851\nEpoch 749/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9971\n\nEpoch 00749: accuracy did not improve from 0.99851\nEpoch 750/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 0.9964\n\nEpoch 00750: accuracy did not improve from 0.99851\nEpoch 751/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 0.9967\n\nEpoch 00751: accuracy did not improve from 0.99851\nEpoch 752/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9971\n\nEpoch 00752: accuracy did not improve from 0.99851\nEpoch 753/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9968\n\nEpoch 00753: accuracy did not improve from 0.99851\nEpoch 754/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9976\n\nEpoch 00754: accuracy did not improve from 0.99851\nEpoch 755/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9972\n\nEpoch 00755: accuracy did not improve from 0.99851\nEpoch 756/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9975\n\nEpoch 00756: accuracy did not improve from 0.99851\nEpoch 757/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 0.9979\n\nEpoch 00757: accuracy did not improve from 0.99851\nEpoch 758/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9969\n\nEpoch 00758: accuracy did not improve from 0.99851\nEpoch 759/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9973\n\nEpoch 00759: accuracy did not improve from 0.99851\nEpoch 760/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.9967\n\nEpoch 00760: accuracy did not improve from 0.99851\nEpoch 761/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9979\n\nEpoch 00761: accuracy did not improve from 0.99851\nEpoch 762/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9977\n\nEpoch 00762: accuracy did not improve from 0.99851\nEpoch 763/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 0.9983\n\nEpoch 00763: accuracy did not improve from 0.99851\nEpoch 764/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.9978\n\nEpoch 00764: accuracy did not improve from 0.99851\nEpoch 765/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9984\n\nEpoch 00765: accuracy did not improve from 0.99851\nEpoch 766/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9978\n\nEpoch 00766: accuracy did not improve from 0.99851\nEpoch 767/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9968\n\nEpoch 00767: accuracy did not improve from 0.99851\nEpoch 768/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9978\n\nEpoch 00768: accuracy did not improve from 0.99851\nEpoch 769/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9978\n\nEpoch 00769: accuracy did not improve from 0.99851\nEpoch 770/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9970\n\nEpoch 00770: accuracy did not improve from 0.99851\nEpoch 771/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9983\n\nEpoch 00771: accuracy did not improve from 0.99851\nEpoch 772/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 0.9987\n\nEpoch 00772: accuracy improved from 0.99851 to 0.99871, saving model to eq_solver.h5\nEpoch 773/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9982\n\nEpoch 00773: accuracy did not improve from 0.99871\nEpoch 774/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9967\n\nEpoch 00774: accuracy did not improve from 0.99871\nEpoch 775/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9979\n\nEpoch 00775: accuracy did not improve from 0.99871\nEpoch 776/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9981\n\nEpoch 00776: accuracy did not improve from 0.99871\nEpoch 777/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9973\n\nEpoch 00777: accuracy did not improve from 0.99871\nEpoch 778/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9978\n\nEpoch 00778: accuracy did not improve from 0.99871\nEpoch 779/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9972\n\nEpoch 00779: accuracy did not improve from 0.99871\nEpoch 780/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9985\n\nEpoch 00780: accuracy did not improve from 0.99871\nEpoch 781/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9982\n\nEpoch 00781: accuracy did not improve from 0.99871\nEpoch 782/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9973\n\nEpoch 00782: accuracy did not improve from 0.99871\nEpoch 783/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9980\n\nEpoch 00783: accuracy did not improve from 0.99871\nEpoch 784/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9979\n\nEpoch 00784: accuracy did not improve from 0.99871\nEpoch 785/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9977\n\nEpoch 00785: accuracy did not improve from 0.99871\nEpoch 786/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.9987\n\nEpoch 00786: accuracy did not improve from 0.99871\nEpoch 787/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9983\n\nEpoch 00787: accuracy did not improve from 0.99871\nEpoch 788/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9976\n\nEpoch 00788: accuracy did not improve from 0.99871\nEpoch 789/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9976\n\nEpoch 00789: accuracy did not improve from 0.99871\nEpoch 790/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9975\n\nEpoch 00790: accuracy did not improve from 0.99871\nEpoch 791/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9971\n\nEpoch 00791: accuracy did not improve from 0.99871\nEpoch 792/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.9971\n\nEpoch 00792: accuracy did not improve from 0.99871\nEpoch 793/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9969\n\nEpoch 00793: accuracy did not improve from 0.99871\nEpoch 794/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9981\n\nEpoch 00794: accuracy did not improve from 0.99871\nEpoch 795/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 0.9963\n\nEpoch 00795: accuracy did not improve from 0.99871\nEpoch 796/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9972\n\nEpoch 00796: accuracy did not improve from 0.99871\nEpoch 797/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9973\n\nEpoch 00797: accuracy did not improve from 0.99871\nEpoch 798/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.9964\n\nEpoch 00798: accuracy did not improve from 0.99871\nEpoch 799/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9950\n\nEpoch 00799: accuracy did not improve from 0.99871\nEpoch 800/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9963\n\nEpoch 00800: accuracy did not improve from 0.99871\nEpoch 801/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 0.9968\n\nEpoch 00801: accuracy did not improve from 0.99871\nEpoch 802/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9981\n\nEpoch 00802: accuracy did not improve from 0.99871\nEpoch 803/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9965\n\nEpoch 00803: accuracy did not improve from 0.99871\nEpoch 804/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9970\n\nEpoch 00804: accuracy did not improve from 0.99871\nEpoch 805/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9979\n\nEpoch 00805: accuracy did not improve from 0.99871\nEpoch 806/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9977\n\nEpoch 00806: accuracy did not improve from 0.99871\nEpoch 807/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9971\n\nEpoch 00807: accuracy did not improve from 0.99871\nEpoch 808/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9974\n\nEpoch 00808: accuracy did not improve from 0.99871\nEpoch 809/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.9966\n\nEpoch 00809: accuracy did not improve from 0.99871\nEpoch 810/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9976\n\nEpoch 00810: accuracy did not improve from 0.99871\nEpoch 811/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9974\n\nEpoch 00811: accuracy did not improve from 0.99871\nEpoch 812/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9972\n\nEpoch 00812: accuracy did not improve from 0.99871\nEpoch 813/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9982\n\nEpoch 00813: accuracy did not improve from 0.99871\nEpoch 814/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9975\n\nEpoch 00814: accuracy did not improve from 0.99871\nEpoch 815/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9973\n\nEpoch 00815: accuracy did not improve from 0.99871\nEpoch 816/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9977\n\nEpoch 00816: accuracy did not improve from 0.99871\nEpoch 817/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.9971\n\nEpoch 00817: accuracy did not improve from 0.99871\nEpoch 818/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9979\n\nEpoch 00818: accuracy did not improve from 0.99871\nEpoch 819/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.9962\n\nEpoch 00819: accuracy did not improve from 0.99871\nEpoch 820/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9971\n\nEpoch 00820: accuracy did not improve from 0.99871\nEpoch 821/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9970\n\nEpoch 00821: accuracy did not improve from 0.99871\nEpoch 822/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.9989\n\nEpoch 00822: accuracy improved from 0.99871 to 0.99891, saving model to eq_solver.h5\nEpoch 823/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9985\n\nEpoch 00823: accuracy did not improve from 0.99891\nEpoch 824/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9967\n\nEpoch 00824: accuracy did not improve from 0.99891\nEpoch 825/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9974\n\nEpoch 00825: accuracy did not improve from 0.99891\nEpoch 826/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9968\n\nEpoch 00826: accuracy did not improve from 0.99891\nEpoch 827/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.9977\n\nEpoch 00827: accuracy did not improve from 0.99891\nEpoch 828/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.9981\n\nEpoch 00828: accuracy did not improve from 0.99891\nEpoch 829/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9980\n\nEpoch 00829: accuracy did not improve from 0.99891\nEpoch 830/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9970\n\nEpoch 00830: accuracy did not improve from 0.99891\nEpoch 831/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9975\n\nEpoch 00831: accuracy did not improve from 0.99891\nEpoch 832/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9974\n\nEpoch 00832: accuracy did not improve from 0.99891\nEpoch 833/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9979\n\nEpoch 00833: accuracy did not improve from 0.99891\nEpoch 834/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9973\n\nEpoch 00834: accuracy did not improve from 0.99891\nEpoch 835/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9972\n\nEpoch 00835: accuracy did not improve from 0.99891\nEpoch 836/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9964\n\nEpoch 00836: accuracy did not improve from 0.99891\nEpoch 837/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9981\n\nEpoch 00837: accuracy did not improve from 0.99891\nEpoch 838/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9966\n\nEpoch 00838: accuracy did not improve from 0.99891\nEpoch 839/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9970\n\nEpoch 00839: accuracy did not improve from 0.99891\nEpoch 840/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9985\n\nEpoch 00840: accuracy did not improve from 0.99891\nEpoch 841/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9970\n\nEpoch 00841: accuracy did not improve from 0.99891\nEpoch 842/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9975\n\nEpoch 00842: accuracy did not improve from 0.99891\nEpoch 843/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9975\n\nEpoch 00843: accuracy did not improve from 0.99891\nEpoch 844/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9964\n\nEpoch 00844: accuracy did not improve from 0.99891\nEpoch 845/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9976\n\nEpoch 00845: accuracy did not improve from 0.99891\nEpoch 846/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9971\n\nEpoch 00846: accuracy did not improve from 0.99891\nEpoch 847/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9966\n\nEpoch 00847: accuracy did not improve from 0.99891\nEpoch 848/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9978\n\nEpoch 00848: accuracy did not improve from 0.99891\nEpoch 849/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9966\n\nEpoch 00849: accuracy did not improve from 0.99891\nEpoch 850/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 0.9969\n\nEpoch 00850: accuracy did not improve from 0.99891\nEpoch 851/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9959\n\nEpoch 00851: accuracy did not improve from 0.99891\nEpoch 852/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9974\n\nEpoch 00852: accuracy did not improve from 0.99891\nEpoch 853/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9972\n\nEpoch 00853: accuracy did not improve from 0.99891\nEpoch 854/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9978\n\nEpoch 00854: accuracy did not improve from 0.99891\nEpoch 855/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9974\n\nEpoch 00855: accuracy did not improve from 0.99891\nEpoch 856/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9972\n\nEpoch 00856: accuracy did not improve from 0.99891\nEpoch 857/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9978\n\nEpoch 00857: accuracy did not improve from 0.99891\nEpoch 858/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9978\n\nEpoch 00858: accuracy did not improve from 0.99891\nEpoch 859/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9963\n\nEpoch 00859: accuracy did not improve from 0.99891\nEpoch 860/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9970\n\nEpoch 00860: accuracy did not improve from 0.99891\nEpoch 861/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9971\n\nEpoch 00861: accuracy did not improve from 0.99891\nEpoch 862/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9977\n\nEpoch 00862: accuracy did not improve from 0.99891\nEpoch 863/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9969\n\nEpoch 00863: accuracy did not improve from 0.99891\nEpoch 864/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 0.9973\n\nEpoch 00864: accuracy did not improve from 0.99891\nEpoch 865/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9978\n\nEpoch 00865: accuracy did not improve from 0.99891\nEpoch 866/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9984\n\nEpoch 00866: accuracy did not improve from 0.99891\nEpoch 867/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9970\n\nEpoch 00867: accuracy did not improve from 0.99891\nEpoch 868/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.9980\n\nEpoch 00868: accuracy did not improve from 0.99891\nEpoch 869/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9981\n\nEpoch 00869: accuracy did not improve from 0.99891\nEpoch 870/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9977\n\nEpoch 00870: accuracy did not improve from 0.99891\nEpoch 871/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9963\n\nEpoch 00871: accuracy did not improve from 0.99891\nEpoch 872/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9978\n\nEpoch 00872: accuracy did not improve from 0.99891\nEpoch 873/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 0.9976\n\nEpoch 00873: accuracy did not improve from 0.99891\nEpoch 874/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 0.9979\n\nEpoch 00874: accuracy did not improve from 0.99891\nEpoch 875/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9979\n\nEpoch 00875: accuracy did not improve from 0.99891\nEpoch 876/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9986\n\nEpoch 00876: accuracy did not improve from 0.99891\nEpoch 877/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9977\n\nEpoch 00877: accuracy did not improve from 0.99891\nEpoch 878/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 0.9984\n\nEpoch 00878: accuracy did not improve from 0.99891\nEpoch 879/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9975\n\nEpoch 00879: accuracy did not improve from 0.99891\nEpoch 880/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9974\n\nEpoch 00880: accuracy did not improve from 0.99891\nEpoch 881/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9975\n\nEpoch 00881: accuracy did not improve from 0.99891\nEpoch 882/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9973\n\nEpoch 00882: accuracy did not improve from 0.99891\nEpoch 883/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9977\n\nEpoch 00883: accuracy did not improve from 0.99891\nEpoch 884/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9970\n\nEpoch 00884: accuracy did not improve from 0.99891\nEpoch 885/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9963\n\nEpoch 00885: accuracy did not improve from 0.99891\nEpoch 886/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.9953\n\nEpoch 00886: accuracy did not improve from 0.99891\nEpoch 887/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9978\n\nEpoch 00887: accuracy did not improve from 0.99891\nEpoch 888/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9973\n\nEpoch 00888: accuracy did not improve from 0.99891\nEpoch 889/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9963\n\nEpoch 00889: accuracy did not improve from 0.99891\nEpoch 890/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9975\n\nEpoch 00890: accuracy did not improve from 0.99891\nEpoch 891/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9981\n\nEpoch 00891: accuracy did not improve from 0.99891\nEpoch 892/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9978\n\nEpoch 00892: accuracy did not improve from 0.99891\nEpoch 893/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9980\n\nEpoch 00893: accuracy did not improve from 0.99891\nEpoch 894/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9975\n\nEpoch 00894: accuracy did not improve from 0.99891\nEpoch 895/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9983\n\nEpoch 00895: accuracy did not improve from 0.99891\nEpoch 896/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.9983\n\nEpoch 00896: accuracy did not improve from 0.99891\nEpoch 897/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.9990\n\nEpoch 00897: accuracy improved from 0.99891 to 0.99901, saving model to eq_solver.h5\nEpoch 898/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.9981\n\nEpoch 00898: accuracy did not improve from 0.99901\nEpoch 899/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9979\n\nEpoch 00899: accuracy did not improve from 0.99901\nEpoch 900/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9976\n\nEpoch 00900: accuracy did not improve from 0.99901\nEpoch 901/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 0.9962\n\nEpoch 00901: accuracy did not improve from 0.99901\nEpoch 902/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9980\n\nEpoch 00902: accuracy did not improve from 0.99901\nEpoch 903/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9968\n\nEpoch 00903: accuracy did not improve from 0.99901\nEpoch 904/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9970\n\nEpoch 00904: accuracy did not improve from 0.99901\nEpoch 905/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9968\n\nEpoch 00905: accuracy did not improve from 0.99901\nEpoch 906/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9966\n\nEpoch 00906: accuracy did not improve from 0.99901\nEpoch 907/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9970\n\nEpoch 00907: accuracy did not improve from 0.99901\nEpoch 908/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9973\n\nEpoch 00908: accuracy did not improve from 0.99901\nEpoch 909/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9979\n\nEpoch 00909: accuracy did not improve from 0.99901\nEpoch 910/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9976\n\nEpoch 00910: accuracy did not improve from 0.99901\nEpoch 911/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9982\n\nEpoch 00911: accuracy did not improve from 0.99901\nEpoch 912/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9985\n\nEpoch 00912: accuracy did not improve from 0.99901\nEpoch 913/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9982\n\nEpoch 00913: accuracy did not improve from 0.99901\nEpoch 914/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9986\n\nEpoch 00914: accuracy did not improve from 0.99901\nEpoch 915/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.9991\n\nEpoch 00915: accuracy improved from 0.99901 to 0.99911, saving model to eq_solver.h5\nEpoch 916/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.9987\n\nEpoch 00916: accuracy did not improve from 0.99911\nEpoch 917/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.9981\n\nEpoch 00917: accuracy did not improve from 0.99911\nEpoch 918/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.9985\n\nEpoch 00918: accuracy did not improve from 0.99911\nEpoch 919/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 0.9987\n\nEpoch 00919: accuracy did not improve from 0.99911\nEpoch 920/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 0.9980\n\nEpoch 00920: accuracy did not improve from 0.99911\nEpoch 921/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 0.9968\n\nEpoch 00921: accuracy did not improve from 0.99911\nEpoch 922/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 0.9962\n\nEpoch 00922: accuracy did not improve from 0.99911\nEpoch 923/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 0.9953\n\nEpoch 00923: accuracy did not improve from 0.99911\nEpoch 924/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.9969\n\nEpoch 00924: accuracy did not improve from 0.99911\nEpoch 925/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9973\n\nEpoch 00925: accuracy did not improve from 0.99911\nEpoch 926/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.9983\n\nEpoch 00926: accuracy did not improve from 0.99911\nEpoch 927/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 0.9984\n\nEpoch 00927: accuracy did not improve from 0.99911\nEpoch 928/1000\n40/40 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 0.9980\n\nEpoch 00928: accuracy did not improve from 0.99911\nEpoch 929/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9980\n\nEpoch 00929: accuracy did not improve from 0.99911\nEpoch 930/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 0.9983\n\nEpoch 00930: accuracy did not improve from 0.99911\nEpoch 931/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.9987\n\nEpoch 00931: accuracy did not improve from 0.99911\nEpoch 932/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.9984\n\nEpoch 00932: accuracy did not improve from 0.99911\nEpoch 933/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.9985\n\nEpoch 00933: accuracy did not improve from 0.99911\nEpoch 934/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9992\n\nEpoch 00934: accuracy improved from 0.99911 to 0.99921, saving model to eq_solver.h5\nEpoch 935/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9987\n\nEpoch 00935: accuracy did not improve from 0.99921\nEpoch 936/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9986\n\nEpoch 00936: accuracy did not improve from 0.99921\nEpoch 937/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.9980\n\nEpoch 00937: accuracy did not improve from 0.99921\nEpoch 938/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9984\n\nEpoch 00938: accuracy did not improve from 0.99921\nEpoch 939/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9974\n\nEpoch 00939: accuracy did not improve from 0.99921\nEpoch 940/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9957\n\nEpoch 00940: accuracy did not improve from 0.99921\nEpoch 941/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9971\n\nEpoch 00941: accuracy did not improve from 0.99921\nEpoch 942/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9969\n\nEpoch 00942: accuracy did not improve from 0.99921\nEpoch 943/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9975\n\nEpoch 00943: accuracy did not improve from 0.99921\nEpoch 944/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9984\n\nEpoch 00944: accuracy did not improve from 0.99921\nEpoch 945/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9977\n\nEpoch 00945: accuracy did not improve from 0.99921\nEpoch 946/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9975\n\nEpoch 00946: accuracy did not improve from 0.99921\nEpoch 947/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9980\n\nEpoch 00947: accuracy did not improve from 0.99921\nEpoch 948/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9966\n\nEpoch 00948: accuracy did not improve from 0.99921\nEpoch 949/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9973\n\nEpoch 00949: accuracy did not improve from 0.99921\nEpoch 950/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9973\n\nEpoch 00950: accuracy did not improve from 0.99921\nEpoch 951/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9986\n\nEpoch 00951: accuracy did not improve from 0.99921\nEpoch 952/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9981\n\nEpoch 00952: accuracy did not improve from 0.99921\nEpoch 953/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9973\n\nEpoch 00953: accuracy did not improve from 0.99921\nEpoch 954/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9973\n\nEpoch 00954: accuracy did not improve from 0.99921\nEpoch 955/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9959\n\nEpoch 00955: accuracy did not improve from 0.99921\nEpoch 956/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9970\n\nEpoch 00956: accuracy did not improve from 0.99921\nEpoch 957/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9968\n\nEpoch 00957: accuracy did not improve from 0.99921\nEpoch 958/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9977\n\nEpoch 00958: accuracy did not improve from 0.99921\nEpoch 959/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9980\n\nEpoch 00959: accuracy did not improve from 0.99921\nEpoch 960/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9979\n\nEpoch 00960: accuracy did not improve from 0.99921\nEpoch 961/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9979\n\nEpoch 00961: accuracy did not improve from 0.99921\nEpoch 962/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9973\n\nEpoch 00962: accuracy did not improve from 0.99921\nEpoch 963/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9978\n\nEpoch 00963: accuracy did not improve from 0.99921\nEpoch 964/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9975\n\nEpoch 00964: accuracy did not improve from 0.99921\nEpoch 965/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.9987\n\nEpoch 00965: accuracy did not improve from 0.99921\nEpoch 966/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9979\n\nEpoch 00966: accuracy did not improve from 0.99921\nEpoch 967/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9971\n\nEpoch 00967: accuracy did not improve from 0.99921\nEpoch 968/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9974\n\nEpoch 00968: accuracy did not improve from 0.99921\nEpoch 969/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9971\n\nEpoch 00969: accuracy did not improve from 0.99921\nEpoch 970/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9977\n\nEpoch 00970: accuracy did not improve from 0.99921\nEpoch 971/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9979\n\nEpoch 00971: accuracy did not improve from 0.99921\nEpoch 972/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9969\n\nEpoch 00972: accuracy did not improve from 0.99921\nEpoch 973/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9982\n\nEpoch 00973: accuracy did not improve from 0.99921\nEpoch 974/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9982\n\nEpoch 00974: accuracy did not improve from 0.99921\nEpoch 975/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9976\n\nEpoch 00975: accuracy did not improve from 0.99921\nEpoch 976/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9987\n\nEpoch 00976: accuracy did not improve from 0.99921\nEpoch 977/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9979\n\nEpoch 00977: accuracy did not improve from 0.99921\nEpoch 978/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9979\n\nEpoch 00978: accuracy did not improve from 0.99921\nEpoch 979/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9970\n\nEpoch 00979: accuracy did not improve from 0.99921\nEpoch 980/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9977\n\nEpoch 00980: accuracy did not improve from 0.99921\nEpoch 981/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9981\n\nEpoch 00981: accuracy did not improve from 0.99921\nEpoch 982/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.9983\n\nEpoch 00982: accuracy did not improve from 0.99921\nEpoch 983/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9978\n\nEpoch 00983: accuracy did not improve from 0.99921\nEpoch 984/1000\n40/40 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 0.9977\n\nEpoch 00984: accuracy did not improve from 0.99921\nEpoch 985/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9971\n\nEpoch 00985: accuracy did not improve from 0.99921\nEpoch 986/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9971\n\nEpoch 00986: accuracy did not improve from 0.99921\nEpoch 987/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9976\n\nEpoch 00987: accuracy did not improve from 0.99921\nEpoch 988/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9975\n\nEpoch 00988: accuracy did not improve from 0.99921\nEpoch 989/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9965\n\nEpoch 00989: accuracy did not improve from 0.99921\nEpoch 990/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9977\n\nEpoch 00990: accuracy did not improve from 0.99921\nEpoch 991/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 0.9988\n\nEpoch 00991: accuracy did not improve from 0.99921\nEpoch 992/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9978\n\nEpoch 00992: accuracy did not improve from 0.99921\nEpoch 993/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9980\n\nEpoch 00993: accuracy did not improve from 0.99921\nEpoch 994/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9972\n\nEpoch 00994: accuracy did not improve from 0.99921\nEpoch 995/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9968\n\nEpoch 00995: accuracy did not improve from 0.99921\nEpoch 996/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9970\n\nEpoch 00996: accuracy did not improve from 0.99921\nEpoch 997/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9968\n\nEpoch 00997: accuracy did not improve from 0.99921\nEpoch 998/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.9971\n\nEpoch 00998: accuracy did not improve from 0.99921\nEpoch 999/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9967\n\nEpoch 00999: accuracy did not improve from 0.99921\nEpoch 1000/1000\n40/40 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9973\n\nEpoch 01000: accuracy did not improve from 0.99921\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(nn['loss'])\nplt.plot(nn['accuracy'])\nplt.title('model loss')\nplt.ylabel('loss and accuracy')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'train_accuracy'], loc='upper right')","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:53:01.853436Z","iopub.execute_input":"2023-09-30T05:53:01.854014Z","iopub.status.idle":"2023-09-30T05:53:02.058459Z","shell.execute_reply.started":"2023-09-30T05:53:01.853977Z","shell.execute_reply":"2023-09-30T05:53:02.057753Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<matplotlib.legend.Legend at 0x78938481f510>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"model = load_model(\"eq_solver.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:53:02.059582Z","iopub.execute_input":"2023-09-30T05:53:02.059947Z","iopub.status.idle":"2023-09-30T05:53:02.155794Z","shell.execute_reply.started":"2023-09-30T05:53:02.059915Z","shell.execute_reply":"2023-09-30T05:53:02.155079Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('../input/d/aatmajajoshi/maskiiiii/22.png',cv2.IMREAD_GRAYSCALE)\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:53:02.156802Z","iopub.execute_input":"2023-09-30T05:53:02.157362Z","iopub.status.idle":"2023-09-30T05:53:02.385963Z","shell.execute_reply.started":"2023-09-30T05:53:02.157327Z","shell.execute_reply":"2023-09-30T05:53:02.385102Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7893846a55d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"if img is not None:\n    img=~img\n    _,thresh=cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n    ctrs,_=cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n    cnt=sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n    w=int(28)\n    h=int(28)\n    train_data=[]\n    print(len(cnt))\n    rects=[]\n    for c in cnt :\n        x,y,w,h= cv2.boundingRect(c)\n        rect=[x,y,w,h]\n        rects.append(rect)\n    bool_rect=[]\n    for r in rects:\n        l=[]\n        for rec in rects:\n            flag=0\n            if rec!=r:\n                if r[0]<(rec[0]+rec[2]+10) and rec[0]<(r[0]+r[2]+10) and r[1]<(rec[1]+rec[3]+10) and rec[1]<(r[1]+r[3]+10):\n                    flag=1\n                l.append(flag)\n            if rec==r:\n                l.append(0)\n        bool_rect.append(l)\n    dump_rect=[]\n    for i in range(0,len(cnt)):\n        for j in range(0,len(cnt)):\n            if bool_rect[i][j]==1:\n                area1=rects[i][2]*rects[i][3]\n                area2=rects[j][2]*rects[j][3]\n                if(area1==min(area1,area2)):\n                    dump_rect.append(rects[i])\n    print(len(dump_rect)) \n    final_rect=[i for i in rects if i not in dump_rect]\n    print(final_rect)\n    for r in final_rect:\n        x=r[0]\n        y=r[1]\n        w=r[2]\n        h=r[3]\n        im_crop =thresh[y:y+h+10,x:x+w+10]\n        im_resize = cv2.resize(im_crop,(28,28))\n        im_resize=np.reshape(im_resize,(28,28,1))\n        train_data.append(im_resize)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:53:02.387116Z","iopub.execute_input":"2023-09-30T05:53:02.387863Z","iopub.status.idle":"2023-09-30T05:53:02.448372Z","shell.execute_reply.started":"2023-09-30T05:53:02.387827Z","shell.execute_reply":"2023-09-30T05:53:02.447669Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"27\n36\n[[8, 159, 11, 8], [8, 121, 11, 8], [8, 83, 11, 8], [8, 45, 10, 8], [30, 11, 339, 160]]\n","output_type":"stream"}]},{"cell_type":"code","source":"equation=''\n\nfor i in range(len(train_data)):\n    \n    train_data[i]=np.array(train_data[i])\n    train_data[i]=train_data[i].reshape(1,28,28,1)\n    result=np.argmax(model.predict(train_data[i]), axis=-1)\n        \n    for j in range(10) :\n        if result[0] == j :\n            equation = equation + str(j)\n    \n    if result[0] == 10 :\n        equation = equation + \"+\"\n    if result[0] == 11 :\n        equation = equation + \"-\"\n    if result[0] == 12 :\n        equation = equation + \"*\"\n    if result[0] == 13 :\n        equation = equation + \"/\"\n    if result[0] == 14 :\n        equation = equation + \"=\"\n    if result[0] == 15 :\n        equation = equation + \".\"\n    if result[0] == 16 :\n        equation = equation + \"x\"\n    if result[0] == 17 :\n        equation = equation + \"y\"      \n    if result[0] == 18 :\n        equation = equation + \"z\"\n    \nprint(\"Your Equation :\", equation)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:53:02.451973Z","iopub.execute_input":"2023-09-30T05:53:02.454150Z","iopub.status.idle":"2023-09-30T05:53:02.925792Z","shell.execute_reply.started":"2023-09-30T05:53:02.454113Z","shell.execute_reply":"2023-09-30T05:53:02.923780Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Your Equation : ----0\n","output_type":"stream"}]}]}